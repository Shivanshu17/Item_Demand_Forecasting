{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input/demand-forecasting-kernels-only/'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"sales_train = pd.read_csv('/kaggle/input/demand-forecasting-kernels-only/train.csv', index_col = 0)\nsales_test = pd.read_csv('/kaggle/input/demand-forecasting-kernels-only/test.csv',index_col = 1)\nsales_train.index = pd.to_datetime(sales_train.index)\nsales_test.index = pd.to_datetime(sales_test.index)\ndisplay(sales_train.sample(10))\ndisplay(sales_test.sample(10))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.graph_objs as go\nimport plotly.offline as py\n\nfrom dateutil.relativedelta import relativedelta\nfrom scipy.optimize import minimize\n\nimport statsmodels.formula.api as smf\nimport statsmodels.tsa.api as smt\nimport statsmodels.api as sm\nimport scipy.stats as sts\nfrom sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error, median_absolute_error, mean_squared_log_error\n\nfrom itertools import product\nfrom tqdm import tqdm_notebook\nimport itertools\n\nimport warnings","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Visualizing Sales & their Properties"},{"metadata":{"trusted":true},"cell_type":"code","source":"# To infer the typical number of items sold each day\nplt.figure(figsize = (15,7))\nplt.hist(sales_train['sales'], bins = 10)\nplt.title('Typical number of items sold each day')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"store_item_df = sales_train.copy()\n# First, let us filterout the required data\nstore_id = 10   # Some store\nitem_id = 40    # Some item\nprint('Before filter:', store_item_df.shape)\nstore_item_df = store_item_df[store_item_df.store == store_id]\nstore_item_df = store_item_df[store_item_df.item == item_id]\nprint('After filter:', store_item_df.shape)\n#display(store_item_df.head())\n\nplt.figure(figsize = (15,7))\nplt.plot(store_item_df.index, store_item_df.sales)\nplt.grid(True)\nplt.title('Daily Item sales at one store')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The daily sales values seem very sporadic, we are now gonna plot the sum value of sales over a week\nstores_sales_df1 = sales_train.copy()\nstores = pd.DataFrame(stores_sales_df1.groupby(['date', 'item']).sum()['sales']).unstack()\nstores = stores.resample('W',label='left').sum()\nstores.sort_index(inplace = True)\n\ndisplay(stores.sample(10))\n\nstores.plot(figsize=(15,7), title='Weekly Item Sales', legend=None)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The daily sales values seem very sporadic, we are now gonna plot the sum value of sales over a week\nstores_sales_df = sales_train.copy()\nstores = pd.DataFrame(stores_sales_df.groupby(['date', 'store']).sum()['sales']).unstack()\nstores = stores.resample('W',label='left').sum()\nstores.sort_index(inplace = True)\n\ndisplay(stores.sample(10))\ndisplay(stores.head(10))\n\nstores.plot(figsize=(15,7), title='Weekly Store Sales', legend=None)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Need to visualise the trends, seasonality and other features (on both additive and multiplicative scales) here.\n\ndate_sales = sales_train.drop(['store','item'], axis=1).copy() \ny = date_sales['sales'].resample('MS').mean() \ny['2017':] #sneak peak\ny.plot(figsize=(15, 7))\n\ndecomposition = sm.tsa.seasonal_decompose(y, model='additive')\ndecomposition.plot()\n\n# Still have to make sense of the plots and describe it, also, need to try and plot these components on a weekly basis, but on a smaller time scale as well (Say 3 months).","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#We can also visualize our data using a method called time-series decomposition that allows us to decompose our time series into three distinct components: \n#trend, seasonality, and noise.\n# Here, we are using multiplicative models of decomposition, instead of additive ones\ndecomposition = sm.tsa.seasonal_decompose(y, model='multiplicative')\ndecomposition.plot()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Supporting Functions\nThese functions will be used to assist creation of data, to implement loss metrics that aren't implicit in scipy library, and a module to plot the results"},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_data(df, store_id, item_id):\n    '''\n    This function creates a series containing the sales values of a particular item of a particular store (as prescribed in the argument values). This will return a pandas series\n    \n    '''\n    new_series = df.copy()\n    new_series = df[df.store == store_id]\n    new_series = df[df.item == item_id]\n    return new_series\n\ndef mean_absolute_percentage_error(y_true, y_pred, multioutput = 'raw_values'):\n    '''\n    This function returns the mean absolute percentage error of the values in form of a numpy array, if the multoutput is set to 'raw_values', and returns a single float value of \n    the average of loss, if the multioutput is set to 'uniform_average'\n    \n    Args:\n        y_true (iterable) -> Representing the actual values of the output\n        y_pred (iterable) -> Representing the predicted values of the output\n        multioutput (string) -> Could either be 'raw_values', or 'uniform_average'\n        \n    Returns:\n        A numpy array or a single float value depending on the multioutput argument.\n    '''\n    if multioutput == 'raw_values':\n        return np.divide(np.abs(y_true - y_pred), np.abs(y_true)) * 100\n    if multioutput == 'uniform_average':\n        return np.mean(np.divide(np.abs(y_true - y_pred), np.abs(y_true)) * 100)\n\ndef total_error(y_true, y_pred, metric):\n    '''\n    This function returns the total error of the predicted values when evaluated against the actual values. It returns a single float variable representing that error\n    \n    '''\n    if metric<0 or metric>4:\n        raise ValueError('The loss metric should be between 1 and 4, included')\n    if metric == 0:\n        error_term = mean_absolute_error(actual_value, pred_value, multioutput = 'uniform_average')\n    if metric == 1:\n        error_term = mean_squared_error(actual_value, pred_value, multioutput = 'uniform_average')\n    if metric == 2: \n        error_term = mean_absolute_percentage_error(actual_value, pred_value, multioutput = 'uniform_average')\n    if metric == 3:\n        error_term = r2(actual_value, pred_value, multioutput = 'uniform_average')\n    if metric == 4:\n        error_term = smape(actual_value, pred_value, multioutput = 'uniform_average')\n    return error_term\n    \n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def smape(y_true, y_pred, multioutput = 'raw_values'):\n    '''\n    This function returns the symmetric mean absolute percentage error of the values in form of a numpy array, if the multoutput is set to 'raw_values', and returns a single float value of \n    the average of loss, if the multioutput is set to 'uniform_average'\n    The formula for SMAPE has been defined here - https://www.forecastpro.com/Trends/forecasting101August2011.html\n    \n    Args:\n        y_true (iterable) -> Representing the actual values of the output\n        y_pred (iterable) -> Representing the predicted values of the output\n        multioutput (string) -> Could either be 'raw_values', or 'uniform_average'\n        \n    Returns:\n        A numpy array or a single float value depending on the multioutput argument.\n        \n    '''\n    if multioutput == 'raw_values':\n        return np.divide(np.abs(y_true - y_pred), np.abs((y_true + y_pred)/2) * 100)\n    else:\n        return np.mean(np.divide(np.abs(y_true - y_pred), np.abs((y_true + y_pred)/2)) * 100)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# A special point of note should be made here, if we have the data, for the actual points (even though we are making predictions for those time steps, and not using the actual data)\n# we can very conveniently use this very function to make a reasonable plot. We will not have to worry about the broadcasting problem in arrays of different sizes, because, we will\n# simply add the know actual values to the actual_value array for the prediction period as well. \n# Also, we need to define seperate plotting functions for each of the models becuase the predictions they make won't necessarily have actual values corresponding to them \n# and we will have to use particular plot methods for each of those methods (for eg, triple exponential smoothing can be predicted using brutlag method)\n\ndef plot_results(pred_value, actual_value, plot_intervals = False, scale = 2, plot_anomalies = False, metric = 4, pred_start = -1):\n    '''\n    This function plots the results of a time series prediction. It can also plot the intervals and anomalies, if so directed.\n    \n    Args:\n        pred_value (iterable) -> Integer values of the predicted values of series\n        actual_value (iterable) -> Integer values of the actual values of the series\n        plot_intervals (boolean) -> Decides if the lower and upper bounds are to be plotted.\n        plot_anomalies (boolean) -> Decides if the anomalies are to be plotted.\n        scale (float) -> An hyperparameter for interval deduction.\n        metric (integer) -> To decide the loss metric to be used\n            0 - MAE\n            1 - MSE\n            2 - MAPE\n            3 - R2\n            4 - SMAPE\n        pred_start (integer) -> A negative integer referring to the number of time steps that have been used as test data. \n        \n    '''\n    \n    plt.figure(figsize = (15,7))\n    plt.title('Prediction values for sales')\n    plt.plot(pred_value, \"g\", label = \"Predicted values\")\n    \n    # Plot the confidence intervals of the smoothed values\n    if plot_intervals:\n        if metric<0 or metric>4:\n            raise ValueError('The loss metric should be between 1 and 4, included')\n        if metric == 0:\n            error_term = mean_absolute_error(actual_value, pred_value, multioutput = 'raw_values')\n        if metric == 1:\n            error_term = mean_squared_error(actual_value, pred_value, multioutput = 'raw_values')\n        if metric == 2: \n            error_term = mean_absolute_percentage_error(actual_value, pred_value, multioutput = 'raw_values')\n        if metric == 3:\n            error_term = r2(actual_value, pred_value, multioutput = 'raw_values')\n        if metric == 4:\n            error_term = smape(actual_value, pred_value, multioutput = 'raw_values')\n        deviation = np.std(actual_value - pred_value)\n        lower_bound = pred_value - (error_term + scale*deviation)\n        upper_bound = pred_value + (error_term + scale*deviation)\n        plt.plot(upper_bound, \"r--\", label = \"Upper Bound / Lower Bound\")\n        plt.plot(lower_bound, \"r--\")\n        \n        if plot_anomalies:\n            anomalies = pd.DataFrame(index = actual_value.index, columns = actual_value.columns)\n            anomalies[actual_value<lower_bound] = actual_value[actual_value<lower_bound]\n            anomalies[actual_value>upper_bound] = actual_value[actual_value>upper_bound]\n            plt.plot(anomalies, \"ro\", makersize = 1)\n        \n    plt.plot(actual_value, label = \"Actual Values\")\n    plt.axvspan(len(actual_value) + pred_start, len(actual_value), alpha = 0.4, color = 'light grey')\n    plt.legend(loc = \"upper left\")\n    # See if the total error term can be added to the plot (to compare the results of different models and different evaluation metrics)\n    plt.grid(True)\n    plt.show()\n        \n        ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Exponential Smoothing\nThe problem with exponential smoothing is that we can't use it to predict the trends of the data for very long periods (6 months in our case), and this limits the utility of this model, nonetheless, this can be used to make a naive prediction on a short term basis and serves as as a good estimate of a conservative prediction which can then be used to compare the volatility and credibility of the predictions made by our long term predictions. Think of it this way, if we were to make predictions for, lets say stock prices, and there is and a certain incident that causes the time series to behave unusually (like a chain of events set in motion due to some scandal or something), then we would want our model to quickly adopt itself to the new environment it has found itself in. Of course, there won't be enough data of such cataclysmic changes to make meaningful long term predictions, and in those scenarios, a manually tuned expectancy value derived from Exponential Smoothing will prove itself to be extremely useful.\n\n#### Smoothing parameters\nWe will try and find the values of the smoothing parameters by passing the input values through a truncated conjugate gradient optmisation algorithm. This will help us ensure that we aren't just tuning those hyperparameters by ourselves, instead they are being set on a formal basis of how beneficial they are to the learning process.\n\nThe code for triple exponential smoothing will be completely different. To provide long term predictions along with deviations that are tuned on the basis of how far long the prediction horizon is. Also, the confidence intervals are derived using the brutlag method, and the parameter values are derived using the optimization function, rather than feeding them manually. I will create a seperate class for these predictions."},{"metadata":{"trusted":true},"cell_type":"code","source":"class Single_ES:\n    \n    \"\"\"\n    This model object will produce the Single Exponential smoothing values. Since it does not have the abilities to either smooth or to add trend, it can only be used to make a single \n    meaningful prediction. That is, just one time step can be predicted.\n    Args:\n        series (iterable) -> Actual time series\n        alpha (float) -> Represents the parameter for single exponential smoothing\n        scaling_factor (float) -> sets the width of the confidence interval\n        \n    Returns:\n        result (iterable) -> A numpy array containing the results of smoothing\n    \n    \"\"\"\n    \n    \n    def __init__(self, series, alpha = 0.9, scaling_factor=2):\n        self.series = series\n        self.alpha = alpha\n        # Scaling factor will only be used, if the plot is done here in this class\n        self.scaling_factor = scaling_factor\n        \n          \n    def single_exponential_smoothing(self):\n        self.result = []\n        self.Smooth = []\n        self.result = [series[0]] # first value is same as series\n        for t in range(1, len(series)):\n            self.result.append(self.alpha * self.series[t] + (1 - self.alpha) * self.result[t-1])\n        # Since we can't make more than one prediction, we can either print the plot here, or we can print the plot from the calling function where the 'model' object is created. \n        # There's no difference in the plots drawn from either.\n        return np.array(self.result)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Double_ES:\n    \n    \"\"\"\n    This class performs double exponential smoothing and updates the deviation, upper bound, and lower bound values for each time step. It also plots the results with its own function.\n    There is no limit to the prediction horizon, however, since we are only relying on the trend update to make the predictions, it wouldn't be wise to set the n_preds value to be too \n    large.\n    \n    Args:\n        series (iterable) -> Actual values array\n        alpha, beta (float) -> Double exponential smoothing parameters\n        n_preds (int) -> predictions horizon\n        scaling_factor (float) - sets the width of the confidence interval and is used to determine the upper bound and lower bound\n    \n    \"\"\"\n    \n    \n    def __init__(self, series, alpha, beta, n_preds, scaling_factor=2):\n        self.series = series\n        self.alpha = alpha\n        self.beta = beta\n        self.n_preds = n_preds\n        self.scaling_factor = scaling_factor\n        \n        \n    def initial_trend(self):\n        sum = 0.0\n        for i in range(self.slen):\n            sum += float(self.series[i+1] - self.series[i])\n        return sum /(len(self.series))\n\n          \n    def double_exponential_smoothing(self):\n        self.result = []\n        self.Smooth = []\n        self.Trend = []\n        self.PredictedDeviation = []\n        self.UpperBound = []\n        self.LowerBound = []\n        \n        seasonals = self.initial_seasonal_components()\n        \n        for i in range(len(self.series)+self.n_preds):\n            if i == 0: # components initialization\n                smooth = self.series[0]\n                trend = self.initial_trend()\n                self.result.append(self.series[0])\n                self.Smooth.append(smooth)\n                self.Trend.append(trend)\n                \n                self.PredictedDeviation.append(0)\n                \n                # Can't decide if I should add the error term (as I did in plot_results() function), into the evaluation of upper and lower bounds. I will have to rewrite that \n                # entire function here, and take metric as an argument as well. Will have to see the plot, to figure out if I should.\n                self.UpperBound.append(self.result[0] + \n                                      self.scaling_factor * \n                                      self.PredictedDeviation[0])\n                \n                self.LowerBound.append(self.result[0] - \n                                      self.scaling_factor * \n                                      self.PredictedDeviation[0])\n                continue\n                \n            if i >= len(self.series): # predicting\n                m = i - len(self.series) + 1\n                \n                # We could apply smoothing to the 'm*trend' part of the code in the next line, it will ensure that the function isn't running just on final trend value, but is \n                # also considering the trend values of the earlier time steps. Will have to see the results to alter this part.\n                self.result.append(smooth + m*trend)\n                \n                # when predicting we increase uncertainty on each step\n                self.PredictedDeviation.append(self.PredictedDeviation[-1]*1.01) \n                \n            else:\n                val = self.series[i]\n                last_smooth, smooth = smooth, self.alpha*(val) + (1-self.alpha)*(smooth+trend)\n                trend = self.beta * (smooth-last_smooth) + (1-self.beta)*trend\n                self.result.append(smooth+trend)\n                \n                # Instead of calculating the standard deviation of the results, we will here calculate the deviation using a variant of brutlag method, with the only difference being,\n                # instead of using gamma as the parameter for deviation updata, we will use a fixed value of 0.5 (which we can adjust according to the results)\n                self.PredictedDeviation.append(0.5 * np.abs(self.series[i] - self.result[i]) \n                                               + (1- 0.5)*self.PredictedDeviation[-1])\n            \n            # Again, the error term can be added here (to grant the model more versatility)\n            self.UpperBound.append(self.result[-1] + \n                                  self.scaling_factor * \n                                  self.PredictedDeviation[-1])\n\n            self.LowerBound.append(self.result[-1] - \n                                  self.scaling_factor * \n                                  self.PredictedDeviation[-1])\n\n            self.Smooth.append(smooth)\n            self.Trend.append(trend)\n        return np.array(self.result)\n        \n    def plot_Double_ES(self, plot_intervals = False, plot_anomalies = False):\n        '''\n        Function plots the results for DES. It can plot the results, lower bound, and upper bound even for time steps that do not have actual values given.\n        This plot function is more geared towards the actual industry situations where we are deploying the model for predictions without having the 'actual observed values' given.\n        \n        args:\n            plot_intervals (boolean) -> Defines if we want to plot the confidence intervals\n            plot_anomalies (boolean) -> Defines if we want to plot the anomalies\n        \n        '''\n        plt.figure(figsize = (15,7))\n        plt.plot(self.results, \"g\", label = \"Predicted Values\")\n        plt.label(\"Prediction values for sales using DES\")\n        \n        if plot_intervals:\n            plt.plot(self.UpperBound, \"r--\", alpha = 0.5, label = \"Upper Bound/Lower Bound\")\n            plt.plot(self.LowerBound, \"r--\", alpha = 0.5)\n            plt.fill_between(x = range(0,len(self.result)), y1 = self.UpperBound, y2 = self.LowerBound, alpha = 0.2, color = \"grey\")\n            \n        if plot_anomalies:\n            self.anomalies = np.array([np.Nan]*len(self.series))\n            self.anomalies[self.series.values<self.LowerBound[:len(self.series)]] = self.series.values[self.series.values<self.LowerBound[:len(self.series)]]\n            self.anomalies[self.series.values>self.UpperBound[:len(self.series)]] = self.series.values[self.series.values>self.UpperBound[:len(self.series)]]\n            plt.plot(self.anomalies, \"o\", markersize = 1, label = \"Anomalies\")\n        \n        plt.plot(self.series, label = \"Actual Values\")\n        plt.legend(loc = 'upper left')\n        plt.axvspan(len(self.series), len(self.result), alpha = 0.4, color = 'light grey')\n        plt.grid(True)\n        plt.axis('tight')\n        plt.show()\n        \n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Triple_ES:\n    \n    \"\"\"\n    In Triple Exponential Smoothing, we implement the Holt-Winters model, and derive the outliers using the brutlag method\n    \n    Args:\n        series (iterbale) -> Actual values array\n        slen (int) -> Length of a season\n        alpha, beta, gamma (float) -> Holt-Winters model coefficients\n        n_preds (int) -> predictions horizon\n        scaling_factor (float) -> sets the width of the confidence interval by Brutlag (usually takes values from 2 to 3)\n    \n    \"\"\"\n    \n    \n    def __init__(self, series, slen, alpha, beta, gamma, n_preds, scaling_factor=1.96):\n        self.series = series\n        self.slen = slen\n        self.alpha = alpha\n        self.beta = beta\n        self.gamma = gamma\n        self.n_preds = n_preds\n        self.scaling_factor = scaling_factor\n        \n        \n    def initial_trend(self):\n        sum = 0.0\n        for i in range(self.slen):\n            sum += float(self.series[i+self.slen] - self.series[i]) / self.slen\n        return sum / self.slen  \n    \n    def initial_seasonal_components(self):\n        seasonals = {}\n        season_averages = []\n        n_seasons = int(len(self.series)/self.slen)\n        # let's calculate season averages\n        for j in range(n_seasons):\n            season_averages.append(sum(self.series[self.slen*j:self.slen*j+self.slen])/float(self.slen))\n        # let's calculate initial values\n        for i in range(self.slen):\n            sum_of_vals_over_avg = 0.0\n            for j in range(n_seasons):\n                sum_of_vals_over_avg += self.series[self.slen*j+i]-season_averages[j]\n            seasonals[i] = sum_of_vals_over_avg/n_seasons\n        return seasonals   \n\n          \n    def triple_exponential_smoothing(self):\n        self.result = []\n        self.Smooth = []\n        self.Season = []\n        self.Trend = []\n        self.PredictedDeviation = []\n        self.UpperBound = []\n        self.LowerBound = []\n        \n        seasonals = self.initial_seasonal_components()\n        \n        for i in range(len(self.series)+self.n_preds):\n            if i == 0: # components initialization\n                smooth = self.series[0]\n                trend = self.initial_trend()\n                self.result.append(self.series[0])\n                self.Smooth.append(smooth)\n                self.Trend.append(trend)\n                self.Season.append(seasonals[i%self.slen])\n                \n                self.PredictedDeviation.append(0)\n                \n                self.UpperBound.append(self.result[0] + \n                                      self.scaling_factor * \n                                      self.PredictedDeviation[0])\n                \n                self.LowerBound.append(self.result[0] - \n                                      self.scaling_factor * \n                                      self.PredictedDeviation[0])\n                continue\n                \n            if i >= len(self.series): # predicting\n                m = i - len(self.series) + 1\n                self.result.append((smooth + m*trend) + seasonals[i%self.slen])\n                \n                # when predicting we increase uncertainty on each step\n                self.PredictedDeviation.append(self.PredictedDeviation[-1]*1.01) \n                \n            else:\n                val = self.series[i]\n                last_smooth, smooth = smooth, self.alpha*(val-seasonals[i%self.slen]) + (1-self.alpha)*(smooth+trend)\n                trend = self.beta * (smooth-last_smooth) + (1-self.beta)*trend\n                seasonals[i%self.slen] = self.gamma*(val-smooth) + (1-self.gamma)*seasonals[i%self.slen]\n                self.result.append(smooth+trend+seasonals[i%self.slen])\n                \n                # Deviation is calculated according to Brutlag algorithm.\n                self.PredictedDeviation.append(self.gamma * np.abs(self.series[i] - self.result[i]) \n                                               + (1-self.gamma)*self.PredictedDeviation[-1])\n                     \n            self.UpperBound.append(self.result[-1] + \n                                  self.scaling_factor * \n                                  self.PredictedDeviation[-1])\n\n            self.LowerBound.append(self.result[-1] - \n                                  self.scaling_factor * \n                                  self.PredictedDeviation[-1])\n\n            self.Smooth.append(smooth)\n            self.Trend.append(trend)\n            self.Season.append(seasonals[i%self.slen])\n        return np.array(self.results)\n    \n    \n    def plot_Triple_ES(self, plot_intervals = False, plot_anomalies = False):\n        '''\n        Function plots the results for DES. It can plot the results, lower bound, and upper bound even for time steps that do not have actual values given.\n        This plot function is more geared towards the actual industry situations where we are deploying the model for predictions without having the 'actual observed values' given.\n        \n        args:\n            plot_intervals (boolean) -> Defines if we want to plot the confidence intervals\n            plot_anomalies (boolean) -> Defines if we want to plot the anomalies\n        \n        '''\n        plt.figure(figsize = (15,7))\n        plt.plot(self.results, \"g\", label = \"Predicted Values\")\n        plt.label(\"Prediction values for sales using Triple Exponential Smoothing\")\n        \n        if plot_intervals:\n            plt.plot(self.UpperBound, \"r--\", alpha = 0.5, label = \"Upper Bound/Lower Bound\")\n            plt.plot(self.LowerBound, \"r--\", alpha = 0.5)\n            plt.fill_between(x = range(0,len(self.result)), y1 = self.UpperBound, y2 = self.LowerBound, alpha = 0.2, color = \"grey\")\n            \n        if plot_anomalies:\n            self.anomalies = np.array([np.Nan]*len(self.series))\n            self.anomalies[self.series.values<self.LowerBound[:len(self.series)]] = self.series.values[self.series.values<self.LowerBound[:len(self.series)]]\n            self.anomalies[self.series.values>self.UpperBound[:len(self.series)]] = self.series.values[self.series.values>self.UpperBound[:len(self.series)]]\n            plt.plot(self.anomalies, \"o\", markersize = 1, label = \"Anomalies\")\n        \n        plt.plot(self.series, label = \"Actual Values\")\n        plt.legend(loc = 'upper left')\n        plt.axvspan(len(self.series), len(self.result), alpha = 0.4, color = 'light grey')\n        plt.grid(True)\n        plt.axis('tight')\n        plt.show()\n        \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# This cell is to include the data preparation part, where we do the train test split on a rolling basis. This function will be called by the proceeding optimization function and this \n# function will call the create_data function created above.\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# This cell will include the main wrapper part of the code, which will first call the optimization function to derive the parameter values, and then will create objects for each of the\n# three types of ES models, and produce the results, then use those objects to call the plot function of TES and normally call the plot function for SES, DES. Here, I will also write \n# the code for identifying the anomalies by calling the necessary functions.\n\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}