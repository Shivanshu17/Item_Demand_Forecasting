{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input/demand-forecasting-kernels-only/'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"sales_train = pd.read_csv('/kaggle/input/demand-forecasting-kernels-only/train.csv', index_col = 0)\nsales_test = pd.read_csv('/kaggle/input/demand-forecasting-kernels-only/test.csv',index_col = 1)\nsales_train.index = pd.to_datetime(sales_train.index)\nsales_test.index = pd.to_datetime(sales_test.index)\ndisplay(sales_train.sample(10))\ndisplay(sales_test.sample(10))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"display(sales_train.info())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"display(sales_train.describe())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.graph_objs as go\nimport plotly.offline as py\n\nfrom dateutil.relativedelta import relativedelta\nfrom scipy.optimize import minimize\n\nimport statsmodels.api as sm\nfrom statsmodels.graphics.tsaplots import plot_acf\nfrom statsmodels.graphics.tsaplots import plot_pacf\nfrom statsmodels.tsa.stattools import adfuller\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom statsmodels.tsa.ar_model import AR\nfrom statsmodels.tsa.arima_model import ARMA, ARIMA\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\nfrom fbprophet import Prophet # Will have to see if I am even gonna use it\n\nimport statsmodels.formula.api as smf\nimport statsmodels.tsa.api as smt\nimport scipy.stats as sts\nfrom sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error, median_absolute_error, mean_squared_log_error\nfrom sklearn.model_selection import TimeSeriesSplit\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom xgboost import XGBRegressor\nimport tensorflow as tf\n\nfrom itertools import product\nfrom tqdm import tqdm_notebook\nimport itertools\n\nimport warnings","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Visualizing Sales & their Properties","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# To infer the typical number of items sold each day\nplt.figure(figsize = (15,7))\nplt.hist(sales_train['sales'], bins = 10)\nplt.title('Typical number of items sold each day')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"store_item_df = sales_train.copy()\n# First, let us filterout the required data\nstore_id = 10   # Some store\nitem_id = 40    # Some item\nprint('Before filter:', store_item_df.shape)\nstore_item_df = store_item_df[store_item_df.store == store_id]\nstore_item_df = store_item_df[store_item_df.item == item_id]\nprint('After filter:', store_item_df.shape)\n#display(store_item_df.head())\n\nplt.figure(figsize = (15,7))\nplt.plot(store_item_df.index, store_item_df.sales)\nplt.grid(True)\nplt.title('Daily Item sales at one store')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The daily sales values seem very sporadic, we are now gonna plot the sum value of sales over a week\nstores_sales_df1 = sales_train.copy()\nstores = pd.DataFrame(stores_sales_df1.groupby(['date', 'item']).sum()['sales']).unstack()\nstores = stores.resample('W',label='left').sum()\nstores.sort_index(inplace = True)\n\ndisplay(stores.sample(10))\n\nstores.plot(figsize=(15,7), title='Weekly Item Sales', legend=None)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The daily sales values seem very sporadic, we are now gonna plot the sum value of sales over a week\nstores_sales_df = sales_train.copy()\nstores = pd.DataFrame(stores_sales_df.groupby(['date', 'store']).sum()['sales']).unstack()\nstores = stores.resample('W',label='left').sum()\nstores.sort_index(inplace = True)\n\ndisplay(stores.sample(10))\ndisplay(stores.head(10))\n\nstores.plot(figsize=(15,7), title='Weekly Store Sales', legend=None)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Need to visualise the trends, seasonality and other features (on both additive and multiplicative scales) here.\n\ndate_sales = sales_train.drop(['store','item'], axis=1).copy() \ny = date_sales['sales'].resample('MS').mean() \ny['2017':] #sneak peak\ny.plot(figsize=(15, 7))\n\ndecomposition = sm.tsa.seasonal_decompose(y, model='additive')\ndecomposition.plot()\n\n# Still have to make sense of the plots and describe it, also, need to try and plot these components on a weekly basis, but on a smaller time scale as well (Say 3 months).","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndecomposition = sm.tsa.seasonal_decompose(y, model='multiplicative')\ndecomposition.plot()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Supporting Functions\nThese functions will be used to assist creation of data, to implement loss metrics that aren't implicit in scipy library, and a module to plot the results","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_data(df, store_id, item_id):\n    '''\n    This function creates a series containing the sales values of a particular item of a particular store (as prescribed in the argument values). This will return a pandas dataframe\n    with datetime index.\n    \n    Args:\n        df (dataframe) -> A multiindex dataframe containing store, item, and dates as index\n        store_id (integer) -> The id of the store\n        item_id (integer) -> The item number\n    \n    '''\n    series = df.loc[(store_id,item_id,slice(None)),:]\n    series_df1 = series.reset_index(['store', 'item'])\n    new_series = series_df1.drop(['store', 'item'], axis = 1)\n    new_series.index = pd.to_datetime(new_series.index)\n    return new_series\n\n\ndef mean_absolute_percentage_error(y_true, y_pred, multioutput = 'raw_values'):\n    '''\n    This function returns the mean absolute percentage error of the values in form of a numpy array, if the multoutput is set to 'raw_values', and returns a single float value of \n    the average of loss, if the multioutput is set to 'uniform_average'\n    \n    Args:\n        y_true (iterable) -> Representing the actual values of the output\n        y_pred (iterable) -> Representing the predicted values of the output\n        multioutput (string) -> Could either be 'raw_values', or 'uniform_average'\n        \n    Returns:\n        A numpy array or a single float value depending on the multioutput argument.\n    '''\n    if multioutput == 'raw_values':\n        return np.divide(np.abs(y_true - y_pred), np.abs(y_true)) * 100\n    if multioutput == 'uniform_average':\n        return np.mean(np.divide(np.abs(y_true - y_pred), np.abs(y_true)) * 100)\n\ndef total_error(actual_value, pred_value, metric):\n    '''\n    This function returns the total error of the predicted values when evaluated against the actual values. It returns a single float variable representing that error\n    \n    '''\n    if metric<0 or metric>4:\n        raise ValueError('The loss metric should be between 1 and 4, included')\n    if metric == 0:\n        error_term = mean_absolute_error(actual_value, pred_value, multioutput = 'uniform_average')\n    if metric == 1:\n        error_term = mean_squared_error(actual_value, pred_value, multioutput = 'uniform_average')\n    if metric == 2: \n        error_term = mean_absolute_percentage_error(actual_value, pred_value, multioutput = 'uniform_average')\n    if metric == 3:\n        error_term = r2(actual_value, pred_value, multioutput = 'uniform_average')\n    if metric == 4:\n        error_term = smape(actual_value, pred_value, multioutput = 'uniform_average')\n    return error_term\n    \n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def smape(y_pred, y_true, multioutput = 'raw_values'):\n    '''\n    This function returns the symmetric mean absolute percentage error of the values in form of a numpy array, if the multoutput is set to 'raw_values', and returns a single float value of \n    the average of loss, if the multioutput is set to 'uniform_average'\n    The formula for SMAPE has been defined here - https://www.forecastpro.com/Trends/forecasting101August2011.html\n    \n    Args:\n        y_true (iterable) -> Representing the actual values of the output\n        y_pred (iterable) -> Representing the predicted values of the output\n        multioutput (string) -> Could either be 'raw_values', or 'uniform_average'\n        \n    Returns:\n        A numpy array or a single float value depending on the multioutput argument.\n        \n    '''\n    if multioutput == 'raw_values':\n        return np.divide(np.abs(y_true - y_pred), np.abs((y_true + y_pred)/2) * 100)\n    else:\n        return np.mean(np.divide(np.abs(y_true - y_pred), np.abs((y_true + y_pred)/2)) * 100)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Also, I need to define seperate plotting functions for each of the models becuase the predictions they make won't necessarily have actual values corresponding to them \n# and we will have to use particular plot methods for each of those methods (for eg, triple exponential smoothing can be predicted using brutlag method)\n\ndef plot_results(pred_value, actual_value, plot_intervals = False, scale = 2, plot_anomalies = False, metric = 4, pred_start = -1):\n    '''\n    This function plots the results of a time series prediction. It can also plot the intervals and anomalies, if so directed.\n    \n    Args:\n        pred_value (iterable) -> Integer values of the predicted values of series\n        actual_value (iterable) -> Integer values of the actual values of the series\n        plot_intervals (boolean) -> Decides if the lower and upper bounds are to be plotted.\n        plot_anomalies (boolean) -> Decides if the anomalies are to be plotted.\n        scale (float) -> An hyperparameter for interval deduction.\n        metric (integer) -> To decide the loss metric to be used\n            0 - MAE\n            1 - MSE\n            2 - MAPE\n            3 - R2\n            4 - SMAPE\n        pred_start (integer) -> A negative integer referring to the number of time steps that have been used as test data. \n        \n    '''\n    \n    plt.figure(figsize = (15,7))\n    plt.title('Prediction values for sales')\n    plt.plot(pred_value, \"g\", label = \"Predicted values\")\n    \n    # Plot the confidence intervals of the smoothed values\n    if plot_intervals:\n        if metric<0 or metric>4:\n            raise ValueError('The loss metric should be between 1 and 4, included')\n        if metric == 0:\n            error_term = mean_absolute_error(actual_value, pred_value, multioutput = 'raw_values')\n        if metric == 1:\n            error_term = mean_squared_error(actual_value, pred_value, multioutput = 'raw_values')\n        if metric == 2: \n            error_term = mean_absolute_percentage_error(actual_value, pred_value, multioutput = 'raw_values')\n        if metric == 3:\n            error_term = r2(actual_value, pred_value, multioutput = 'raw_values')\n        if metric == 4:\n            error_term = smape(actual_value, pred_value, multioutput = 'raw_values')\n        deviation = np.std(actual_value - pred_value)\n        lower_bound = pred_value - (error_term + scale*deviation)\n        upper_bound = pred_value + (error_term + scale*deviation)\n        plt.plot(upper_bound, \"r--\", label = \"Upper Bound / Lower Bound\")\n        plt.plot(lower_bound, \"r--\")\n        \n        if plot_anomalies:\n            anomalies = pd.DataFrame(index = actual_value.index, columns = actual_value.columns)\n            anomalies[actual_value<lower_bound] = actual_value[actual_value<lower_bound]\n            anomalies[actual_value>upper_bound] = actual_value[actual_value>upper_bound]\n            plt.plot(anomalies, \"ro\", makersize = 1)\n        \n    plt.plot(actual_value, label = \"Actual Values\")\n    plt.axvspan(len(actual_value) + pred_start, len(actual_value), alpha = 0.6, color = 'grey')\n    plt.legend(loc = \"upper left\")\n    # See if the total error term can be added to the plot (to compare the results of different models and different evaluation metrics)\n    plt.grid(True)\n    plt.show()\n        \n        ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Exponential Smoothing\nThe problem with exponential smoothing is that we can't use it to predict the trends of the data for very long periods (6 months in our case), and this limits the utility of this model, nonetheless, this can be used to make a naive prediction on a short term basis and serves as as a good estimate of a conservative prediction which can then be used to compare the volatility and credibility of the predictions made by our long term predictions. Think of it this way, if we were to make predictions for, lets say stock prices, and there is and a certain incident that causes the time series to behave unusually (like a chain of events set in motion due to some scandal or something), then we would want our model to quickly adopt itself to the new environment it has found itself in. Of course, there won't be enough data of such cataclysmic changes to make meaningful long term predictions, and in those scenarios, a manually tuned expectancy value derived from Exponential Smoothing will prove itself to be extremely useful.\n\n#### Smoothing parameters\nWe will try and find the values of the smoothing parameters by passing the input values through a truncated conjugate gradient optmisation algorithm. This will help us ensure that we aren't just tuning those hyperparameters by ourselves, instead they are being set on a formal basis of how beneficial they are to the learning process.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class Single_ES:\n    \n    \"\"\"\n    This model object will produce the Single Exponential smoothing values. Since it does not have the abilities to either smooth or to add trend, it can only be used to make a single \n    meaningful prediction. That is, just one time step can be predicted.\n    Args:\n        series (iterable) -> Actual time series\n        alpha (float) -> Represents the parameter for single exponential smoothing\n        scaling_factor (float) -> sets the width of the confidence interval\n        \n    Returns:\n        result (iterable) -> A numpy array containing the results of smoothing\n    \n    \"\"\"\n    \n    \n    def __init__(self, series, alpha = 0.9, scaling_factor=2):\n        self.series = series\n        self.alpha = alpha\n        # Scaling factor will only be used, if the plot is done here in this class\n        self.scaling_factor = scaling_factor\n        \n          \n    def single_exponential_smoothing(self):\n        self.result = []\n        self.Smooth = []\n        self.result = [series[0]] # first value is same as series\n        for t in range(1, len(series)):\n            self.result.append(self.alpha * self.series[t] + (1 - self.alpha) * self.result[t-1])\n        # Since we can't make more than one prediction, we can either print the plot here, or we can print the plot from the calling function where the 'model' object is created. \n        # There's no difference in the plots drawn from either.\n        return np.array(self.result)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Double_ES:\n    \n    \"\"\"\n    This class performs double exponential smoothing and updates the deviation, upper bound, and lower bound values for each time step. It also plots the results with its own function.\n    There is no limit to the prediction horizon, however, since we are only relying on the trend update to make the predictions, it wouldn't be wise to set the n_preds value to be too \n    large.\n    \n    Args:\n        series (iterable) -> Actual values array\n        alpha, beta (float) -> Double exponential smoothing parameters\n        n_preds (int) -> predictions horizon\n        scaling_factor (float) - sets the width of the confidence interval and is used to determine the upper bound and lower bound\n    \n    \"\"\"\n    \n    \n    def __init__(self, series, alpha, beta, n_preds, scaling_factor=1):\n        if type(series) is np.ndarray:\n            series = pd.DataFrame(series)\n        self.series = series\n        self.alpha = alpha\n        self.beta = beta\n        self.n_preds = n_preds\n        self.scaling_factor = scaling_factor\n        \n        \n    def initial_trend(self):\n        sum = 0.0\n        for i in range(len(self.series)-1):\n            sum += float(self.series.iloc[i+1] - self.series.iloc[i])\n        return sum /(len(self.series))\n\n          \n    def double_exponential_smoothing(self):\n        self.result = []\n        self.Smooth = []\n        self.Trend = []\n        self.PredictedDeviation = []\n        self.UpperBound = []\n        self.LowerBound = []\n        \n        for i in range(len(self.series)+self.n_preds):\n            if i == 0: # components initialization\n                smooth = self.series.iloc[0]\n                trend = self.initial_trend()\n                self.result.append(self.series.iloc[0])\n                self.Smooth.append(smooth)\n                self.Trend.append(trend)\n                \n                self.PredictedDeviation.append(0)\n                \n                # Can't decide if I should add the error term (as I did in plot_results() function), into the evaluation of upper and lower bounds. I will have to rewrite that \n                # entire function here, and take metric as an argument as well. Will have to see the plot, to figure out if I should.\n                self.UpperBound.append(self.result[0] + \n                                      self.scaling_factor * \n                                      self.PredictedDeviation[0])\n                \n                self.LowerBound.append(self.result[0] - \n                                      self.scaling_factor * \n                                      self.PredictedDeviation[0])\n                continue\n                \n            if i >= len(self.series): # predicting\n                m = i - len(self.series) + 1\n                \n                # We could apply smoothing to the 'm*trend' part of the code in the next line, it will ensure that the function isn't running just on final trend value, but is \n                # also considering the trend values of the earlier time steps. Will have to see the results to alter this part.\n                self.result.append(smooth + m*trend)\n                \n                # when predicting we increase uncertainty on each step\n                self.PredictedDeviation.append(self.PredictedDeviation[-1]*1.01) \n                \n            else:\n                val = self.series.iloc[i]\n                last_smooth, smooth = smooth, self.alpha*(val) + (1-self.alpha)*(smooth+trend)\n                trend = self.beta * (smooth-last_smooth) + (1-self.beta)*trend\n                self.result.append(smooth+trend)\n                \n                # Instead of calculating the standard deviation of the results, we will here calculate the deviation using a variant of brutlag method, with the only difference being,\n                # instead of using gamma as the parameter for deviation updata, we will use a fixed value of 0.5 (which we can adjust according to the results)\n                self.PredictedDeviation.append(0.5 * np.abs(self.series.iloc[i] - self.result[i]) \n                                               + (1- 0.5)*self.PredictedDeviation[-1])\n            \n            # Again, the error term can be added here (to grant the model more versatility)\n            self.UpperBound.append(self.result[-1] + \n                                  self.scaling_factor * \n                                  self.PredictedDeviation[-1])\n\n            self.LowerBound.append(self.result[-1] - \n                                  self.scaling_factor * \n                                  self.PredictedDeviation[-1])\n\n            self.Smooth.append(smooth)\n            self.Trend.append(trend)\n\n        \n    def plot_Double_ES(self, test_data, plot_intervals = True, plot_anomalies = True, aggregated = True):\n        '''\n        Function plots the results for DES. It can plot the results, lower bound, and upper bound even for time steps that do not have actual values given.\n        This plot function is more geared towards the actual industry situations where we are deploying the model for predictions without having the 'actual observed values' given.\n        \n        args:\n            test_data (dataframe) -> Dataframe representing the last 180 values indexed according to their date time index values\n            plot_intervals (boolean) -> Defines if we want to plot the confidence intervals\n            plot_anomalies (boolean) -> Defines if we want to plot the anomalies\n            aggregated (boolean) -> Determines whether or not we want to resample our plots on a weekly basis\n        \n        '''\n        self.result = pd.DataFrame(np.array(self.result))\n        any_df = pd.concat([self.series, test_data], axis = 0)\n        self.result.index = pd.to_datetime(any_df.index)\n        t = self.series.index\n        l = len(self.series)\n        self.series = pd.concat([self.series, test_data], axis = 0)\n        \n        if plot_intervals:\n            self.UpperBound = pd.DataFrame(self.UpperBound)\n            self.UpperBound.index = self.result.index\n            self.LowerBound = pd.DataFrame(self.LowerBound)\n            self.LowerBound.index = self.result.index\n            \n        if plot_anomalies:\n            self.anomalies = np.array([np.NaN]*l)\n            self.anomalies = pd.DataFrame(self.anomalies)\n            self.anomalies.index = t\n            drop_list = []\n            \n            for i in range(len(self.anomalies)):\n                if self.series.values[i][0] < self.LowerBound.values[i][0]:\n                    self.anomalies.iloc[i] = self.series.values[i][0]\n                if self.series.values[i][0] > self.UpperBound.values[i][0]:\n                    self.anomalies.iloc[i] = self.series.values[i][0]\n                else:\n                    drop_list.append(i)\n            \n            self.anomalies.drop(self.anomalies.index[drop_list], axis = 0, inplace = True)\n        \n        # This part prints the results in an aggregated form and everything is resampled according to weekly basis\n        if aggregated == True:\n            results_aggregated = self.result.resample('W', label = 'left').sum()\n            actual_aggregated = self.series.resample('W', label = 'left').sum()\n            UpperBound_aggregated = self.UpperBound.resample('W', label = 'left').sum()\n            LowerBound_aggregated = self.LowerBound.resample('W', label = 'left').sum()\n            \n            # The following code is to find aggregated anomalies. \n            l1 = len(results_aggregated)\n            anomalies_aggregated = np.array([np.NaN]*l1)\n            anomalies_aggregated = pd.DataFrame(anomalies_aggregated)\n            anomalies_aggregated.index = results_aggregated.index\n            drop_list1 = []\n            for j in range(len(anomalies_aggregated)):\n                if actual_aggregated.values[j][0] < LowerBound_aggregated.values[j][0]:\n                    anomalies_aggregated.iloc[j] = actual_aggregated.values[j][0]\n                if actual_aggregated.values[j][0] > UpperBound_aggregated.values[j][0]:\n                    anomalies_aggregated.iloc[j] = actual_aggregated.values[j][0]\n                else:\n                    drop_list1.append(j)\n            anomalies_aggregated.drop(anomalies_aggregated.index[drop_list1], axis = 0, inplace = True)\n\n            plt.figure(figsize = (15,7))\n            plt.plot(results_aggregated, \"g\", label =\"Predicted Values\")\n            plt.plot(actual_aggregated, \"b\", label = \"Actual Values\")\n            plt.plot(UpperBound_aggregated, \"r--\", alpha = 0.3, label = \"Upper Bound/ Lower Bound\")\n            plt.plot(LowerBound_aggregated, \"r--\")\n            plt.fill_between(x = results_aggregated.index, y1 = UpperBound_aggregated.values.ravel(), y2 = LowerBound_aggregated.values.ravel(), alpha = 0.2, color = \"grey\")\n            plt.axvspan(test_data.index[0], test_data.index[-1], alpha = 0.4, color = \"grey\")\n            plt.plot(anomalies_aggregated, \"o\", color = \"r\", markersize = 5, label = \"Anomalies\")\n            plt.grid(True)\n            plt.legend(loc = \"upper left\")\n            plt.show()\n        \n        # The following part prints the results without aggregation\n        else:\n            plt.figure(figsize = (15,7))\n            plt.plot(self.result, \"g\", label = \"Predicted Values\")\n            plt.title(\"Prediction values for sales using Triple Exponential Smoothing\")\n            plt.plot(self.series, \"b\", label = \"Actual Values\")\n            plt.plot(self.UpperBound, \"r--\", alpha = 0.3, label = \"Upper Bound/ Lower Bound\")\n            plt.plot(self.LowerBound, \"r--\", alpha = 0.3)\n            plt.fill_between(x = self.result.index, y1 = self.UpperBound.values.ravel(), y2 = self.LowerBound.values.ravel(), alpha = 0.2, color = \"grey\")\n            plt.axvspan(test_data.index[0], test_data.index[-1], alpha = 0.6, color = 'grey')\n            plt.plot(self.anomalies, \"o\", color = \"r\", markersize = 5, label = \"Anomalies\")\n            plt.show()       \n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Triple_ES:\n    \n    \"\"\"\n    In Triple Exponential Smoothing, we implement the Holt-Winters model, and derive the outliers using the brutlag method\n    \n    Args:\n        series (iterbale) -> Actual values array\n        slen (int) -> Length of a season\n        alpha, beta, gamma (float) -> Holt-Winters model coefficients\n        n_preds (int) -> predictions horizon\n        scaling_factor (float) -> sets the width of the confidence interval by Brutlag (usually takes values from 2 to 3)\n    \n    \"\"\"\n    \n    \n    def __init__(self, series, alpha, beta, gamma, n_preds = 180, scaling_factor=1.5, slen = 365):\n        if type(series) is np.ndarray:\n            series = pd.DataFrame(series)\n        self.series = series\n        self.slen = int(slen)\n        self.alpha = alpha\n        self.beta = beta\n        self.gamma = gamma\n        self.n_preds = n_preds\n        self.scaling_factor = scaling_factor\n        \n        \n    def initial_trend(self):\n        sum = 0.0\n        for i in range(self.slen):\n            sum += float(self.series.iloc[i+self.slen] - self.series.iloc[i]) / self.slen\n        return sum / self.slen  \n    \n    def initial_seasonal_components(self):\n        seasonals = {}\n        season_averages = []\n        n_seasons = int(len(self.series)/self.slen)\n        # let's calculate season averages\n        for j in range(n_seasons):\n            season_averages.append((self.series.iloc[self.slen*j:self.slen*j+self.slen].sum()[0])/float(self.slen))\n        # let's calculate initial values\n        for i in range(self.slen):\n            sum_of_vals_over_avg = 0.0\n            for j in range(n_seasons):\n                sum_of_vals_over_avg += self.series.iloc[self.slen*j+i]-season_averages[j]\n            seasonals[i] = sum_of_vals_over_avg/n_seasons\n        return seasonals   \n\n          \n    def triple_exponential_smoothing(self):\n        self.result = []\n        self.Smooth = []\n        self.Season = []\n        self.Trend = []\n        self.PredictedDeviation = []\n        self.UpperBound = []\n        self.LowerBound = []\n        \n        seasonals = self.initial_seasonal_components()\n        \n        for i in range(len(self.series)+self.n_preds):\n            if i == 0: # components initialization\n                smooth = self.series.iloc[0]\n                trend = self.initial_trend()\n                self.result.append(self.series.iloc[0])\n                self.Smooth.append(smooth)\n                self.Trend.append(trend)\n                self.Season.append(seasonals[i%self.slen])\n                \n                self.PredictedDeviation.append(0)\n                \n                self.UpperBound.append(self.result[0] + \n                                      self.scaling_factor * \n                                      self.PredictedDeviation[0])\n                \n                self.LowerBound.append(self.result[0] - \n                                      self.scaling_factor * \n                                      self.PredictedDeviation[0])\n                continue\n                \n            if i >= len(self.series): # predicting\n                m = i - len(self.series) + 1\n                self.result.append((smooth + m*trend) + seasonals[i%self.slen])\n                \n                # when predicting we increase uncertainty on each step\n                self.PredictedDeviation.append(self.PredictedDeviation[-1]*1.01) \n                \n            else:\n                val = self.series.iloc[i]\n                last_smooth, smooth = smooth, self.alpha*(val-seasonals[i%self.slen]) + (1-self.alpha)*(smooth+trend)\n                trend = self.beta * (smooth-last_smooth) + (1-self.beta)*trend\n                seasonals[i%self.slen] = self.gamma*(val-smooth) + (1-self.gamma)*seasonals[i%self.slen]\n                self.result.append(smooth+trend+seasonals[i%self.slen])\n                \n                # Deviation is calculated according to Brutlag algorithm.\n                self.PredictedDeviation.append(self.gamma * np.abs(self.series.iloc[i] - self.result[i]) \n                                               + (1-self.gamma)*self.PredictedDeviation[-1])\n                     \n            self.UpperBound.append(self.result[-1] + \n                                  self.scaling_factor * \n                                  self.PredictedDeviation[-1])\n\n            self.LowerBound.append(self.result[-1] - \n                                  self.scaling_factor * \n                                  self.PredictedDeviation[-1])\n\n            self.Smooth.append(smooth)\n            self.Trend.append(trend)\n            self.Season.append(seasonals[i%self.slen])\n\n    \n    \n    def plot_Triple_ES(self, test_data, plot_intervals = True, plot_anomalies = True, aggregated = True):\n        '''\n        Function plots the results for TES. It can plot the results, lower bound, and upper bound even for time steps that do not have actual values given.\n        This plot function is more geared towards the actual industry situations where we are deploying the model for predictions without having the 'actual observed values' given.\n        \n        args:\n            test_data (dataframe) -> Datetime index wise representation of the last 180 day values.\n            plot_intervals (boolean) -> Defines if we want to plot the confidence intervals\n            plot_anomalies (boolean) -> Defines if we want to plot the anomalies\n            aggregated (boolena) -> Decides whether or not we want to resample our output on a weekly basis\n        \n        '''\n        self.result = pd.DataFrame(self.result)\n        any_df = pd.concat([self.series, test_data], join = \"inner\", axis = 0)\n        self.result.index = pd.to_datetime(any_df.index)\n        t = self.series.index\n        l = len(self.series)\n        self.series = pd.concat([self.series, test_data], axis = 0)\n        #display(self.series.tail(20))\n        #display(test_data.tail(20))\n        #display(self.result)\n        \n        if plot_intervals:\n            self.UpperBound = pd.DataFrame(self.UpperBound)\n            self.UpperBound.index = self.result.index\n            self.LowerBound = pd.DataFrame(self.LowerBound)\n            self.LowerBound.index = self.result.index\n\n        if plot_anomalies:\n            self.anomalies = np.array([np.NaN]*(l))\n            self.anomalies = pd.DataFrame(self.anomalies)\n            self.anomalies.index = t\n            drop_list = []\n            \n            for i in range(len(self.anomalies)):\n                if self.series.values[i][0] < self.LowerBound.values[i][0]:\n                    self.anomalies.iloc[i] = self.series.values[i][0]\n                if self.series.values[i][0] > self.UpperBound.values[i][0]:\n                    self.anomalies.iloc[i] = self.series.values[i][0]\n                else:\n                    drop_list.append(i)\n            \n            self.anomalies.drop(self.anomalies.index[drop_list], axis = 0, inplace = True)\n            \n        # This part prints the results in an aggregated form and everything is resampled according to weekly basis\n        if aggregated == True:\n            results_aggregated = self.result.resample('W', label = 'left').sum()\n            actual_aggregated = self.series.resample('W', label = 'left').sum()\n            UpperBound_aggregated = self.UpperBound.resample('W', label = 'left').sum()\n            LowerBound_aggregated = self.LowerBound.resample('W', label = 'left').sum()\n            \n            # The following code is to find aggregated anomalies. \n            l1 = len(results_aggregated)\n            anomalies_aggregated = np.array([np.NaN]*l1)\n            anomalies_aggregated = pd.DataFrame(anomalies_aggregated)\n            anomalies_aggregated.index = results_aggregated.index\n            drop_list1 = []\n            for j in range(len(anomalies_aggregated)):\n                if actual_aggregated.values[j][0] < LowerBound_aggregated.values[j][0]:\n                    anomalies_aggregated.iloc[j] = actual_aggregated.values[j][0]\n                if actual_aggregated.values[j][0] > UpperBound_aggregated.values[j][0]:\n                    anomalies_aggregated.iloc[j] = actual_aggregated.values[j][0]\n                else:\n                    drop_list1.append(j)\n            anomalies_aggregated.drop(anomalies_aggregated.index[drop_list1], axis = 0, inplace = True)\n            \n\n            plt.figure(figsize = (15,7))\n            plt.plot(results_aggregated, \"g\", label =\"Predicted Values\")\n            plt.plot(actual_aggregated, \"b\", label = \"Actual Values\")\n            plt.plot(UpperBound_aggregated, \"r--\", alpha = 0.3, label = \"Upper Bound/ Lower Bound\")\n            plt.plot(LowerBound_aggregated, \"r--\")\n            plt.fill_between(x = results_aggregated.index, y1 = UpperBound_aggregated.values.ravel(), y2 = LowerBound_aggregated.values.ravel(), alpha = 0.2, color = \"grey\")\n            plt.axvspan(test_data.index[0], test_data.index[-1], alpha = 0.4, color = \"grey\")\n            plt.plot(anomalies_aggregated, \"o\", color = \"r\", markersize = 5, label = \"Anomalies\")\n            plt.grid(True)\n            plt.legend(loc = \"upper left\")\n            plt.show()\n        \n        # The following part prints the results without aggregation\n        else:\n            plt.figure(figsize = (15,7))\n            plt.plot(self.result, \"g\", label = \"Predicted Values\")\n            plt.title(\"Prediction values for sales using Triple Exponential Smoothing\")\n            plt.plot(self.series, \"b\", label = \"Actual Values\")\n            plt.plot(self.UpperBound, \"r--\", alpha = 0.3, label = \"Upper Bound/ Lower Bound\")\n            plt.plot(self.LowerBound, \"r--\", alpha = 0.3)\n            plt.fill_between(x = self.result.index, y1 = self.UpperBound.values.ravel(), y2 = self.LowerBound.values.ravel(), alpha = 0.2, color = \"grey\")\n            plt.axvspan(test_data.index[0], test_data.index[-1], alpha = 0.6, color = 'grey')\n            plt.plot(self.anomalies, \"o\", color = \"r\", markersize = 5, label = \"Anomalies\")\n            plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# This cell is to include the data preparation part, where we do the train test split on a rolling basis. This function will be called by the proceeding optimization function.\n\ndef desCVscore(params, series, loss_function=mean_squared_error):\n    \"\"\"\n        Returns error on CV  and thereby helps decide the best parameters for Double Exponential Smoothing\n        \n        Args:\n        \n            params -> vector of parameters for optimization\n            series -> dataset with timeseries\n        \n    \"\"\"\n    # errors array\n    errors = []\n    \n    values = series.values\n    alpha, beta = params\n    \n    # set the number of folds for cross-validation\n    tscv = TimeSeriesSplit(n_splits=9) \n    \n    # iterating over folds, train model on each, forecast and calculate error\n    for train, test in tscv.split(values):\n\n        model = Double_ES(series=values[train], alpha=alpha, beta=beta, n_preds=len(test))\n        model.double_exponential_smoothing()\n        \n        predictions = model.result[-len(test):]\n        actual = values[test]\n        error = loss_function(predictions, actual)\n        errors.append(error)\n        \n    return np.mean(np.array(errors))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def tesCVscore(params, series, loss_function=mean_squared_error, slen=365):\n    \"\"\"\n        Returns error on CV  and thereby helps decide the best parameters for Triple Exponential Smoothing\n        \n        Args:\n            params -> vector of parameters for optimization\n            series -> dataset with timeseries\n            slen -> season length for Triple Exponential Smoothing model. Here the season length is obviously 365.\n    \"\"\"\n    # errors array\n    errors = []\n    \n    values = series.values\n    alpha, beta, gamma = params\n    \n    # set the number of folds for cross-validation\n    tscv = TimeSeriesSplit(n_splits=9) \n    \n    # iterating over folds, train model on each, forecast and calculate error\n    for train, test in tscv.split(values):\n\n        model = Triple_ES(series=values[train], slen=slen, \n                            alpha=alpha, beta=beta, gamma=gamma, n_preds=len(test))\n        model.triple_exponential_smoothing()\n        \n        predictions = model.result[-len(test):]\n        actual = values[test]\n        error = loss_function(predictions, actual)\n        errors.append(error)\n        \n    return np.mean(np.array(errors))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def des_forecast(train_data, test_data):\n    '''\n    \n    \n    \n    '''\n    #Initializing model parameters, alpha and beta\n    x_des = [0,0]\n\n    # Minimizing the loss function. Still have to figure out a way to use MAPE loss function in this metric as well\n    #opt_des = minimize(desCVscore, x0 = x_des, args = (train_data, mean_squared_error), method = \"TNC\", bounds = ((0,1), (0,1)))\n\n    # Finding optimal parameter values\n    #alpha, beta = opt_des.x\n    alpha = 0.90\n    beta = 0.001\n    print(\"Alpha value is: \", alpha)\n    print(\"Beta value is:\", beta)\n\n    # Now we train the model with these values of alpha and beta\n    model_des = Double_ES(train_data, alpha, beta, n_preds = 180, scaling_factor = 1)\n    model_des.double_exponential_smoothing()\n    model_des.plot_Double_ES(test_data, plot_intervals = True, plot_anomalies = True)\n    anomalies_des = model_des.anomalies\n    result_des = model_des.result\n    error_des = total_error(test_data.values.ravel(), result_des[-180:].values.ravel(), 1)\n    print(\"The total error using mean squared error is\", error_des)\n    display(anomalies_des)\n    return result_des, anomalies_des","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def tes_forecast(train_data, test_data):\n    '''\n    \n    \n    \n    \n    '''\n    # Initializing model parameters, alpha and beta and gamma\n    x_tes = [0,0,0]\n\n    # Minimizing the loss function. Still have to figure out a way to use SMAPE loss function in this metric as well\n    #opt_tes = minimize(tesCVscore, x0 = x_tes, args = (train_data, mean_squared_error), method = \"TNC\", bounds = ((0,1), (0,1),(0,1)))\n\n    # Finding optimal parameter values\n    #apha, beta, gamma = opt_tes.x\n    alpha = 0.9\n    beta = 0.0001\n    gamma = 0.9\n    print(\"Alpha value is: \", alpha)\n    print(\"Beta value is:\", beta)\n    print(\"Gamma Value is : \", gamma)\n\n    # Now we train the model with these values of alpha, beta and gamma\n    \n    model_tes = Triple_ES(train_data, alpha, beta, gamma, slen = 365, n_preds = 180, scaling_factor = 0.9)\n    model_tes.triple_exponential_smoothing()\n    model_tes.plot_Triple_ES(test_data, plot_intervals = True, plot_anomalies = True)\n    anomalies_tes = model_tes.anomalies\n    result_tes = model_tes.result\n    error_tes = total_error(test_data.values.ravel(), result_tes[-180:].values.ravel(), 1)\n    #UpperBound_tes = model_tes.UpperBound\n    #LowerBound_tes = model_tes.LowerBound\n    display(anomalies_tes)\n    print(\"The total error using mean squared error is\", error_tes)\n    #display(result_tes[-180:])\n    #display(test_data)\n    #total_data = pd.concat([train_data, test_data], axis = 0)\n    \n    return result_tes, anomalies_tes\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Actual Implementation of Exponential Smoothing","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# This cell will include the main wrapper part of the code, which will first call the data creation function followed by a call to the optimization function to derive the parameter values, and then will create objects for each of the\n# three types of ES models, and produce the results, then use those objects to call the plot function of TES and DES. Here, I will also write \n# the code for identifying the anomalies by calling the necessary functions. \n\nstore_chain_train = pd.read_csv('/kaggle/input/demand-forecasting-kernels-only/train.csv')\nstorewise_train = store_chain_train.set_index(['store', 'item', 'date'])\n\n#The following part can be put into a loop for all stores and all items. I won't be doing it for now\nstore_id = 10     #Random store id\nitem_id = 40      # Random item id\nseries_data = create_data(storewise_train, store_id, item_id)\ntrain_data = series_data.iloc[:-180]\ntest_data = series_data.iloc[-180:]\n#display(train_data)\n#display(test_data)\n\n\n# The following code is for Double Exponential Smoothing. The function defintion des_forecast is to house the following code\nresults_des, anomalies_des = des_forecast(train_data, test_data)\n#plot_results(results_des, series_data, plot_intervals = True, scale = 2, plot_anomalies = True, metric = 4, pred_start = -180)\n\n\n# The following code is for Triple Exponential Smoothing.\nresults_tes, anomalies_tes = tes_forecast(train_data, test_data)\n#plot_results(results_tes, series_data, plot_intervals = False, scale = 2, plot_anomalies = False, metric = 4, pred_start = -180)\n\n\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## SARIMA Modelling\n* We will check for stationarity first using Augmented Dickey Fuller test\n* Then, we will remove trend and seasonality to make the sales data stationary\n* Then, identify the p, d, q values (Parameters for ARIMA) using the SARIMAX optimization function\n* Then, we fit the SARIMA model using the metrics thus defined","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Here we will create a weekly aggregated version of our time series. We will be testing our results on both these TS\nstore_id = 10\nitem_id = 40\nsales_df = pd.read_csv('/kaggle/input/demand-forecasting-kernels-only/train.csv')\nsales_df = sales_df.set_index(['store','item','date'])\nsales_df = create_data(sales_df, store_id, item_id)\nsales_df_train = sales_df.iloc[:-180]\nsales_df_test = sales_df.iloc[-180:]\nsales_df_agg = sales_df.resample('W', label = 'left').sum()\nsales_df_agg_train = sales_df_agg.iloc[:-26]\nsales_df_agg_test = sales_df_agg.iloc[-26:]\n#display(sales_df.head(10))\n#display(sales_df_agg.head(10))\n#display(sales_df.describe())\n#display(sales_df_agg.describe())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1. Check For stationarity","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def test_stationarity(y, lags=None):\n    '''\n        This function will plot the time series, its ACF and PACF, & calculate Augmented Dickeyâ€“Fuller test\n        \n        y (DataFrame) -> Actual sales timeseries. With 'sales' as the only column\n        lags (integer) -> Indicates the number of lags to include in ACF, PACF calculation\n    '''\n    #if not isinstance(y, pd.Series):\n        #y = pd.Series(y, index = ['sales'])\n        \n   \n    fig = plt.figure(figsize=(15,7))\n    layout = (2, 2)\n    ts_ax = plt.subplot2grid(layout, (0, 0), colspan=2)\n    acf_ax = plt.subplot2grid(layout, (1, 0))\n    pacf_ax = plt.subplot2grid(layout, (1, 1))\n        \n    y.plot(ax=ts_ax)\n    adf_test = adfuller(y.sales, autolag = 'AIC')\n    adf_output = pd.Series(adf_test[0:4], index = ['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n    display(adf_output)\n    ts_ax.set_title('Time Series Analysis Plots\\n Dickey-Fuller: p={0:.5f}'.format(adf_output[1]))\n    smt.graphics.plot_acf(y, lags=lags, ax=acf_ax)\n    smt.graphics.plot_pacf(y, lags=lags, ax=pacf_ax)\n    plt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_stationarity(sales_df_train, lags = 60)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_stationarity(sales_df_train, lags = 375)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2. Remove Non Stationarity\nSince the Dickey Fuller statistic isn't big enough to reject the null hypothesis - presence of a unit root, we therefore conclude the series is not stationary, and there is some trend in the series, along with seasonality (as is obvious from the plot). \nCheck out acceptable test statistic scores for rejecting null hypothesis at - https://www.statisticshowto.com/adf-augmented-dickey-fuller-test/\n\nI will try:\n* Differencing to remove trend and seasonality.\n* If trend isn't removed even by differencing, then I will try a bit of smoothing","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Here we will try to remove non stationarity by removing seasonality. We can see the season is an year -> 365 days.\nsales_df_train_diff1 = sales_df_train - sales_df_train.shift(365)\ntest_stationarity(sales_df_train_diff1.iloc[365:], lags = 60)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*While the dickey fuller value does not indicate the presence of a unit root, the plot above indicates a slight correlation within the lag values on a weekly basis (Notice the consistent high value on ACF plot on every 7th day). This makes sense as well, sales data is should (probably) show a weekly cycle. This could perhaps be eased out by 1st order differencing with a lag of 7 (to represent a week)*","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_df_train_diff2 = sales_df_train_diff1 - sales_df_train_diff1.shift(7)\ntest_stationarity(sales_df_train_diff2.iloc[372:], lags = 60)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*Now, our Time Series is perfectly stationary, and therefore represents something that does not depend on the time at all. We can now safely feed this series into the SARIMA model*","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### SARIMA Instantiation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"class SARIMA:\n    '''\n    This class calculates the best parameter value, forecasts future values on the derived set of parameters and plots the results\n    \n    Args:\n        initial_values (iterable) -> List representing the range of the parameters in order - ps, qs, Ps, Qs (two values for each)\n        series (DataFrame) -> Train timeseries of sales\n        n_preds (integer) -> The number of predictions\n        D (integer) -> Seasonality difference parameter\n        d (integer) -> Trend difference index\n        slen (integer) -> Length of the season\n    \n    '''\n    def __init__(self, series, initial_values, n_preds = 180, D = 1, d = 7, slen = 365):\n        self.series = series\n        self.initial_values = initial_values\n        self.n_preds = n_preds\n        self.d = d\n        self.D = D\n        self.slen = slen\n        \n        \n    def tune_SARIMA(self):\n        '''\n        This function finds the best combination of parameters for SARIMA model\n        \n        Returns:\n            result_table (dataframe) -> The combination of Parameters along with their respective AIC values as calculated by SARIMAX\n        \n        '''\n        \n        '''\n        # I have to comment out the entire tuning section because the cell was taking too long to run\n        \n        self.result_table = []\n        ps = range(self.initial_values[0], self.initial_values[1])\n        qs = range(self.initial_values[2], self.initial_values[3])\n        Ps = range(self.initial_values[4], self.initial_values[5])\n        Qs = range(self.initial_values[6], self.initial_values[7])\n        parameters_list = list(product(ps,qs,Ps,Qs))        \n        best_aic = float(\"inf\")\n        for param in tqdm_notebook(parameters_list):\n            # we need try-except because on some combinations model fails to converge\n            try:\n                model=sm.tsa.statespace.SARIMAX(self.series.sales, order=(param[0], self.d, param[1]), \n                                                seasonal_order=(param[2], self.D, param[3], self.slen)).fit(disp=-1)\n            except:\n                continue\n            aic = model.aic\n            # saving best model, AIC and parameters\n            if aic < best_aic:\n                best_model = model\n                best_aic = aic\n                best_param = param\n            results.append([param, model.aic])\n\n        self.result_table = pd.DataFrame(results)\n        self.result_table.columns = ['parameters', 'aic']\n        # sorting in ascending order, the lower AIC is - the better\n        self.result_table = self.result_table.sort_values(by='aic', ascending=True).reset_index(drop=True)\n        p,q,P,Q = self.result_table.parameters[0]\n        '''\n        self.result_table = []\n        self.result_table.append(7)\n        self.result_table.append(7)\n        self.result_table.append(2)\n        self.result_table.append(1)\n        # print('The best parameter values are : p - ', p, \" q - \", q, 'P - ', P, ' Q - ',Q)\n        \n    \n    def fit_SARIMA(self, test_data):\n        '''\n        This function fits the parameters and makes a forecast\n        Also, this function can be used to determine the anomalies as well. I will havet to figure out how that can \n        be done in SARIMA modeling, but that shouldn't be too tough.\n        \n        Args:\n            test_data (DataFrame) -> test part of the sales timeseries\n        \n        '''\n        self.tune_SARIMA()\n        self.forecast = []\n        #p, q, P, Q = self.result_table.parameters[0]\n        p = self.result_table[0]\n        q = self.result_table[1]\n        P = self.result_table[2]\n        Q = self.result_table[3]\n        best_model=sm.tsa.statespace.SARIMAX(self.series.sales, order=(p, self.d, q), seasonal_order=(P, self.D, Q, self.slen)).fit(disp=-1)\n        print(best_model.summary())\n        a_df = pd.concat(self.series, test_data, join = 'inner', axis = 0)\n        forecast = best_model.predict(start = (self.slen+self.d), end = self.series.shape[0]+self.n_preds)\n        self.forecast = np.array((np.NaN)*(self.slen + self.d))\n        self.forecast = self.forecast.append(forecast)\n        self.forecast = pd.DataFrame(self.forecast, columns = 'sales')\n        self.forecast.index = pd.to_datetime(a_df.index)\n        \n        \n    def plot_SARIMA(self, actual_values, aggregated = True):\n        '''\n        This function plots the results derived from SARIMA modelling. I still have to add the code for Anomalies.\n        \n        Args:\n            actual_values (DataFrame) -> Timeseries representing sales_df, with test set included.\n            aggregated (Boolean) -> Determines if we want to print the aggregated scores of values\n        \n        '''\n        error_pred = mean_squared_error(actual_values['sales'][0-self.n_preds:], self.forecast['sales'][0-self.n_preds:])\n        plt.figure(figsize = (15,7))\n        plt.title(\"Mean Squared Error: {0:.2f}%\".format(error_pred))\n        if aggregated:\n            agg_forecast = self.forecast.resample('W', label = 'left').sum() # Have to see if I should handle NaN values with fillna\n            agg_actual_values = actual_values.resample('W', label = 'left').sum()\n            plt.plot(agg_forecast, color = \"g\", label = \"Predicted Values\")\n            plt.plot(agg_actual_values, color = 'b', label = \"Actual Values\")\n        else:\n            plt.plot(self.forecast, color = \"g\", label = \"Predicted Values\")\n            plt.plot(actual_values, color = \"b\", label = \"Actual Values\")\n        plt.axvspan(self.forecast.index[0-self.n_preds], self.forecast.index[-1], alpha = 0.4, color = 'grey')\n        plt.legend(loc = 'upper left')\n        plt.grid(True)\n        plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# This function will create the object for SARIMA model and call the plot_SARIMA function as well\n\ninitial_values = [3,8,4,8,1,3,1,2]\n\nmodel_SARIMA = SARIMA(sales_df_train, initial_values, slen = 365, n_preds = 180)\nmodel_SARIMA.fit_SARIMA(sales_df_test)\nresult_SARIMA = model_SARIMA.forecast\nmodel_SARIMA.plot_SARIMA(sales_df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Regression Part\nThis part is divided into \n* Feature Generation\n* Model Training\n* Ploting Results","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### 1) Feature Generation\nWe are gonna try out the following three things\n* Lags of time series itself\n* Adding Window Statistics (Max/Min/Mean/Variance of window)\n* Day of the week, Day of the month, Day of the quarter, is it a holiday, did something special happen this day, etc\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from datetime import date\nimport holidays\n\nus_holidays = holidays.UnitedStates()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nstore_sale_train = pd.read_csv('/kaggle/input/demand-forecasting-kernels-only/train.csv')\nstorewise_train = store_sale_train.set_index(['store', 'item', 'date'])\nstore_id = 10     #Random store id\nitem_id = 40      # Random item id\nstore_item_sale_TS = create_data(storewise_train, store_id, item_id)\nstore_item_sale_TS_train= store_item_sale_TS.iloc[:-180]\nstore_item_sale_TS_test = store_item_sale_TS.iloc[-180:]\nstore_item_sale_df_not_TS = store_item_sale_TS.reset_index()\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 1) Adding Date Related Features","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"store_item_sale_TS_0 = store_item_sale_TS.copy()\nstore_item_sale_TS_0['day_of_year'] = 0\nstore_item_sale_TS_0['week_of_year'] = 0\nstore_item_sale_TS_0['month_of_year'] = 0\nstore_item_sale_TS_0['day_of_month'] = 0\nstore_item_sale_TS_0['day_of_week'] = 0\nstore_item_sale_TS_0['weekend'] = 0\nstore_item_sale_TS_0['holiday'] = 0\n\nfor date in store_item_sale_TS_0.index:\n    store_item_sale_TS_0.loc[date, 'day_of_year'] = date.dayofyear\n    store_item_sale_TS_0.loc[date, 'week_of_year'] = date.weekofyear\n    store_item_sale_TS_0.loc[date, 'month_of_year'] = date.month\n    store_item_sale_TS_0.loc[date, 'day_of_month'] = date.day\n    store_item_sale_TS_0.loc[date, 'day_of_week'] = date.dayofweek\n    if date in us_holidays:\n        store_item_sale_TS_0.loc[date,'holiday'] = 1\n    if date.dayofweek == 6 or date.dayofweek == 5:\n        store_item_sale_TS_0.loc[date,'weekend'] = 1\n    \ndisplay(store_item_sale_TS_0.head())\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 2) Adding Lag Features","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"store_item_sale_TS_1 = store_item_sale_TS_0.copy()\nfor i in range(365,365+28):\n    store_item_sale_TS_1[\"lag_{}\".format(i)] = store_item_sale_TS_1.sales.shift(i)\ndisplay(store_item_sale_TS_1.head())\ndisplay(store_item_sale_TS_1.tail())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 3) Adding Window Features (Upto one lag week)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"month_lag_colname_list = [\"lag_{}\".format(i + 365) for i in range(0,28)]\nweek1_lag_colname_list = [\"lag_{}\".format(i + 365) for i in range(0,7)]\nweek2_lag_colname_list = [\"lag_{}\".format(i + 365) for i in range(7,14)]\nweek3_lag_colname_list = [\"lag_{}\".format(i + 365) for i in range(14,21)]\nweek4_lag_colname_list = [\"lag_{}\".format(i + 365) for i in range(21,28)]\nfortnight_lag_colname_list = [\"lag_{}\".format(i + 365) for i in range(0,14)]\n\nstore_item_sale_TS_2 = store_item_sale_TS_1.copy()\n\nstore_item_sale_TS_2['month_lag_mean'] = 0\nstore_item_sale_TS_2['month_lag_max'] = 0\nstore_item_sale_TS_2['month_lag_min'] = 0\nstore_item_sale_TS_2['month_lag_variance'] = 0\n\nstore_item_sale_TS_2['week1_lag_mean'] = 0\nstore_item_sale_TS_2['week1_lag_max'] = 0\nstore_item_sale_TS_2['week1_lag_min'] = 0\nstore_item_sale_TS_2['week1_lag_variance'] = 0\n\n# display(month_lag_colname_list)\n\nfor date in store_item_sale_TS_2.index:\n    month_lag_series = store_item_sale_TS_2.loc[date, month_lag_colname_list]\n    week1_lag_series = store_item_sale_TS_2.loc[date, week1_lag_colname_list]\n    \n    store_item_sale_TS_2.loc[date, 'month_lag_mean'] = month_lag_series.mean()\n    store_item_sale_TS_2.loc[date, 'month_lag_max'] = month_lag_series.max()\n    store_item_sale_TS_2.loc[date, 'month_lag_min'] = month_lag_series.min()\n    store_item_sale_TS_2.loc[date, 'month_lag_variance'] = month_lag_series.var()\n    \n    store_item_sale_TS_2.loc[date,'week1_lag_mean'] = week1_lag_series.mean()\n    store_item_sale_TS_2.loc[date,'week1_lag_max'] = week1_lag_series.max()\n    store_item_sale_TS_2.loc[date,'week1_lag_min'] = week1_lag_series.min()\n    store_item_sale_TS_2.loc[date,'week1_lag_variance'] = week1_lag_series.var()\n\ndisplay(store_item_sale_TS_2.tail(5))\n\n\n# display(sale_TS.tail(10))\n# display(sale_TS_1.tail(10))\n\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 4) All the window features","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"store_item_sale_TS_3 = store_item_sale_TS_2.copy()\n\nstore_item_sale_TS_3['week2_lag_mean'] = 0\nstore_item_sale_TS_3['week2_lag_max'] = 0\nstore_item_sale_TS_3['week2_lag_min'] = 0\nstore_item_sale_TS_3['week2_lag_variance'] = 0\n\nstore_item_sale_TS_3['week3_lag_mean'] = 0\nstore_item_sale_TS_3['week3_lag_max'] = 0\nstore_item_sale_TS_3['week3_lag_min'] = 0\nstore_item_sale_TS_3['week3_lag_variance'] = 0\n\nstore_item_sale_TS_3['week4_lag_mean'] = 0\nstore_item_sale_TS_3['week4_lag_max'] = 0\nstore_item_sale_TS_3['week4_lag_min'] = 0\nstore_item_sale_TS_3['week4_lag_variance'] = 0\n\n# display(month_lag_colname_list)\n\nfor date in store_item_sale_TS_3.index:\n    fortnight_lag_series = store_item_sale_TS_3.loc[date, fortnight_lag_colname_list]\n    week2_lag_series = store_item_sale_TS_3.loc[date, week2_lag_colname_list]\n    week3_lag_series = store_item_sale_TS_3.loc[date, week3_lag_colname_list]\n    week4_lag_series = store_item_sale_TS_3.loc[date, week4_lag_colname_list]\n    \n    store_item_sale_TS_3.loc[date, 'fortnight_lag_mean'] = fortnight_lag_series.mean()\n    store_item_sale_TS_3.loc[date, 'fortnight_lag_max'] = fortnight_lag_series.max()\n    store_item_sale_TS_3.loc[date, 'fortnight_lag_min'] = fortnight_lag_series.min()\n    store_item_sale_TS_3.loc[date, 'fortnight_lag_variance'] = fortnight_lag_series.var()\n    \n    store_item_sale_TS_3.loc[date,'week2_lag_mean'] = week2_lag_series.mean()\n    store_item_sale_TS_3.loc[date,'week2_lag_max'] = week2_lag_series.max()\n    store_item_sale_TS_3.loc[date,'week2_lag_min'] = week2_lag_series.min()\n    store_item_sale_TS_3.loc[date,'week2_lag_variance'] = week2_lag_series.var()\n    \n    store_item_sale_TS_3.loc[date,'week3_lag_mean'] = week3_lag_series.mean()\n    store_item_sale_TS_3.loc[date,'week3_lag_max'] = week3_lag_series.max()\n    store_item_sale_TS_3.loc[date,'week3_lag_min'] = week3_lag_series.min()\n    store_item_sale_TS_3.loc[date,'week3_lag_variance'] = week3_lag_series.var()\n    \n    store_item_sale_TS_3.loc[date,'week4_lag_mean'] = week4_lag_series.mean()\n    store_item_sale_TS_3.loc[date,'week4_lag_max'] = week4_lag_series.max()\n    store_item_sale_TS_3.loc[date,'week4_lag_min'] = week4_lag_series.min()\n    store_item_sale_TS_3.loc[date,'week4_lag_variance'] = week4_lag_series.var()\n\ndisplay(store_item_sale_TS_3.tail())\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2) Model Training & Plot\nWe are to do the following:\n1. Create Test-Train splits\n2. Standardizing the scale (if it is required)\n3. Training the model and producing the results\n    * By Linear Regression\n    * By XGBoost Regressor\n4. Plot the results","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"#### 1. Test Train Splits","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def timeseries_train_test_split(X, y, test_size):\n    \"\"\"\n        Perform train-test split with respect to time series structure\n    \"\"\"\n    test_size = 0 - test_size\n    X_train = X.iloc[:test_size]\n    y_train = y.iloc[:test_size]\n    X_test = X.iloc[test_size:]\n    y_test = y.iloc[test_size:]\n    \n    return X_train, X_test, y_train, y_test","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### 2.   Scaling Functions","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def standard_scaler(X_train, X_test):\n    scaler = StandardScaler()\n    X_train_scaled = scaler.fit_transform(X_train)\n    X_test_scaled = scaler.transform(X_test)\n    #display(X_train_scaled)\n    #display(X_test_scaled.shape)\n    return X_train_scaled, X_test_scaled\n\ndef minmax_scaler(X_train, X_test):\n    minmaxscaler = MinMaxScaler()\n    X_train_scaled = minmaxscaler.fit_transform(X_train)\n    X_test_scaled = minmaxscaler.transform(X_test)\n    #display(X_train_minmax_scaled)\n    #display(X_test_minmax_scaled.shape)\n    return X_train_scaled, X_test_scaled","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def lr_model(info_level = 2, test_size = 180, scaling_metric = 1, plot_intervals = True, plot_anomalies = False, scale = 0.2):\n    '''\n    This function trains the Linear Regression model on whichever level of data it is assigned and plots the results.\n    \n    '''\n    \n    if info_level == 1:\n        data_df = store_item_sale_TS_1.copy()\n    if info_level == 2:\n        data_df = store_item_sale_TS_2.copy()\n    if info_level == 3:\n        data_df = store_item_sale_TS_3.copy()\n    \n    # First we get the data in the required format\n    data_df = data_df.dropna()\n    data_df_y = data_df['sales']\n    data_df_X = data_df.drop(['sales'], axis = 1)\n    X_train, X_test, y_train, y_test = timeseries_train_test_split(data_df_X, data_df_y, test_size = test_size)\n    \n    # Then we scale the data\n    if scaling_metric == 1:\n        X_train_scaled, X_test_scaled = standard_scaler(X_train, X_test)\n    if scaling_metric == 2:\n        X_train_scaled, X_test_scaled = minmax_scaler(X_train, X_test)\n    \n    # We now initialize the model and train it\n    lr = LinearRegression()\n    lr.fit(X_train_scaled, y_train)\n    prediction = lr.predict(X_test_scaled)\n    #display(prediction)\n    \n    \n    # We are now gonna plot the results along with coefficient scores\n    \n    predicted_df = pd.DataFrame(prediction, columns = ['sales'])\n    predicted_df.index = X_test.index\n    # Total error\n    error_pred = float(mean_squared_error(data_df['sales'][0-test_size:],predicted_df['sales']))\n    \n    plt.figure(figsize = (15,7))\n    plt.title(\"Mean Squared Error {0:.2f}\".format(error_pred))\n    agg_predicted = predicted_df.resample('W', label = 'left').sum()\n    agg_actual = data_df.resample('W', label = 'left').sum()\n    plt.plot(agg_predicted, color = \"g\", label = \"Predicted Values\")\n    plt.plot(agg_actual['sales'], color = \"b\", label = \"Actual Values\")\n    \n    if plot_intervals:\n        deviation = []\n        lower_bound = []\n        upper_bound = []\n        agg_actual_reindexed = agg_actual.set_index(pd.Index(range(0,len(agg_actual))))\n        agg_predicted_reindexed = agg_predicted.set_index(pd.Index(range(0,len(agg_predicted))))\n        deviation = np.std((agg_actual_reindexed.iloc[(0-len(agg_predicted)):, 0].values, agg_predicted_reindexed['sales'].values)) \n        lower_bound = np.array(agg_predicted['sales'].values) - (error_pred + scale*deviation)\n        upper_bound = np.array(agg_predicted['sales'].values) + (error_pred + scale*deviation)\n        LowerBound_aggregated = pd.DataFrame(lower_bound, columns = ['sales'])\n        LowerBound_aggregated.index = agg_predicted.index\n        UpperBound_aggregated = pd.DataFrame(upper_bound, columns = ['sales'])\n        UpperBound_aggregated.index = agg_predicted.index\n        plt.plot(UpperBound_aggregated['sales'], \"r--\", alpha = 0.3, label = \"Upper Bound/ Lower Bound\")\n        plt.plot(LowerBound_aggregated['sales'], \"r--\")\n        plt.fill_between(x = agg_predicted.index, y1 = UpperBound_aggregated.values.ravel(), y2 = LowerBound_aggregated.values.ravel(), alpha = 0.2, color = \"grey\")\n        plt.axvspan(X_test.index[0], X_test.index[-1], alpha = 0.4, color = \"grey\")\n        \n    # I still have to write the code for anomalies\n    plt.legend(loc = 'upper left')\n    plt.grid(True)\n    plt.show()\n    \n    return lr\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def XGBoost_model(info_level = 2, test_size = 180, scaling_metric = 1, plot_intervals = True, plot_anomalies = False, scale = 1.25):\n    '''\n    This function trains the Linear Regression model on whichever level of data it is assigned and plots the results.\n    \n    '''\n    \n    if info_level == 1:\n        data_df = store_item_sale_TS_1.copy()\n    if info_level == 2:\n        data_df = store_item_sale_TS_2.copy()\n    if info_level == 3:\n        data_df = store_item_sale_TS_3.copy()\n    \n    # First we get the data in the required format\n    data_df = data_df.dropna()\n    data_df_y = data_df['sales']\n    data_df_X = data_df.drop(['sales'], axis = 1)\n    X_train, X_test, y_train, y_test = timeseries_train_test_split(data_df_X, data_df_y, test_size = test_size)\n    \n    # Then we scale the data\n    if scaling_metric == 1:\n        X_train_scaled, X_test_scaled = standard_scaler(X_train, X_test)\n    if scaling_metric == 2:\n        X_train_scaled, X_test_scaled = minmax_scaler(X_train, X_test)\n    \n    # We now initialize the model and train it\n    xgb = XGBRegressor()\n    xgb.fit(X_train_scaled, y_train)\n    prediction = xgb.predict(X_test_scaled)\n    #display(prediction)\n    \n    \n    # We are now gonna plot the results along with coefficient scores\n    \n    predicted_df = pd.DataFrame(prediction, columns = ['sales'])\n    predicted_df.index = X_test.index\n    # Total error\n    error_pred = float(mean_squared_error(data_df['sales'][0-test_size:],predicted_df['sales']))\n    \n    plt.figure(figsize = (15,7))\n    plt.title(\"Mean Squared Error {0:.2f}\".format(error_pred))\n    agg_predicted = predicted_df.resample('W', label = 'left').sum()\n    agg_actual = data_df.resample('W', label = 'left').sum()\n    plt.plot(agg_predicted, color = \"g\", label = \"Predicted Values\")\n    plt.plot(agg_actual['sales'], color = \"b\", label = \"Actual Values\")\n    \n    if plot_intervals:\n        deviation = []\n        lower_bound = []\n        upper_bound = []\n        agg_actual_reindexed = agg_actual.set_index(pd.Index(range(0,len(agg_actual))))\n        agg_predicted_reindexed = agg_predicted.set_index(pd.Index(range(0,len(agg_predicted))))\n        deviation = np.std((agg_actual_reindexed.iloc[(0-len(agg_predicted)):, 0].values, agg_predicted_reindexed['sales'].values)) \n        lower_bound = np.array(agg_predicted['sales'].values) - ((error_pred/10) + scale*deviation)\n        upper_bound = np.array(agg_predicted['sales'].values) + ((error_pred/10) + scale*deviation)\n        LowerBound_aggregated = pd.DataFrame(lower_bound, columns = ['sales'])\n        LowerBound_aggregated.index = agg_predicted.index\n        UpperBound_aggregated = pd.DataFrame(upper_bound, columns = ['sales'])\n        UpperBound_aggregated.index = agg_predicted.index\n        plt.plot(UpperBound_aggregated['sales'], \"r--\", alpha = 0.3, label = \"Upper Bound/ Lower Bound\")\n        plt.plot(LowerBound_aggregated['sales'], \"r--\")\n        plt.fill_between(x = agg_predicted.index, y1 = UpperBound_aggregated.values.ravel(), y2 = LowerBound_aggregated.values.ravel(), alpha = 0.2, color = \"grey\")\n        plt.axvspan(X_test.index[0], X_test.index[-1], alpha = 0.4, color = \"grey\")\n        \n    # I still have to write the code for anomalies\n    plt.legend(loc = 'upper left')\n    plt.grid(True)\n    plt.show()\n    \n    return xgb\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr = lr_model()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb = XGBoost_model()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr = lr_model(info_level = 3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb = XGBoost_model(info_level = 3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## LSTM Approach\nHere we are gonna perform the following steps:\n1. Create the dataset\n2. Create the test train split along with making the data ready for LSTM.\n3. Modelling Approaches (Give reasons for how different models could influence the results)\n4. Plot the results","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Questions to be Answered :-\n1. Is there any difference in feeding lag data in form of features (by adding columns containing the lag information, and using window_size = 1) or by feeding the model with \"windows of time\" using 'history_size' feature?\n2. Should the 'history_size' parameter be greater than the 'target_size' parameter (in case of multistep predictions). Either way, what's the acceptable approach to identifying both these values for Time Series forecasting? Also, why do we need to defining 'history_size' and 'window_size' in the first place, how does it relate to the workings of LSTM?\n3. (Optional) Is there a reasonable way of using one step predictions to be used as lag features in upcoming step predictions, instead of having to make the model learn to make multi step predictions (of size 'target_size'), as this task of multi-step predictions comes with accompanying data size requirments.\n4. Can the data from all the series from all the stores and items be used (since all of it seems to follow a pattern)? What is a reasonable standard for this? How to manage the time scale of each subsequent time series (if that would even be a problem, since we are not actually feeding the model with dates)?\n5. What are the reasonable values for the number of 'cells' or 'units' in LSTM? number of epochs? Number of layers? Droput value? The shape of input and output? How to infer them and initialize them in Keras? \n6. Can the lag features from a year ago be used in the batch formalization of input for LSTM (if so, how)? How are the results influenced by this in comparison to the ones produced by immediate lag values? What is a reasonable 'ratio' of the data in comparison to seasons, such that seasonal lag values can be safely used?\n7. How would stacking recurrent layers effect the results? How does using bi-directional RNNs effect the output?\n8. How does the shuffle function on 'from_tensor_slices' effect the ouput. I have to try both ways, with and without it.\n___\n8. How to handle seasonality & trend in data?\n9. How does LSTM clustering work? Is it any good for our case? When should it be applied?\n10. How to use CNNs along with LSTM for forecasting problems? When are they preferred? List the scenarios where they might be really effective (more so than general LSTM)\n11. How does LSTNet work and what are its ideal use case scenarios?\n\n","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"store_sale_train = pd.read_csv('/kaggle/input/demand-forecasting-kernels-only/train.csv')\nstorewise_train = store_sale_train.set_index(['store', 'item', 'date'])\nstore_id = 10     #Random store id\nitem_id = 40      # Random item id\nstore_item_sale_TS = create_data(storewise_train, store_id, item_id)\nstore_item_sale_TS_train= store_item_sale_TS.iloc[:-180]\nstore_item_sale_TS_test = store_item_sale_TS.iloc[-180:]\nstore_item_sale_df_not_TS = store_item_sale_TS.reset_index()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 1. Create Dataset\nI am gonna try the following datasets\n1. The simple time series of sales of a single store, to be used:\n     * With window size of 50 containing immediate lag values.\n     * With window size of 60 containing 30 immediate lag values and 30 year prior lag values.\n___    \n2. A simple 50 lag TS data (added as features/columns), to be used with windows of size 1, such that:\n    * Data belongs to 50 immediate lags.\n    * Data belongs to 30 immediate lags and 30 lags from the year prior\n___    \n3. Create a time series with seasonality and trend components removed (using PACF plots). Try it on whichever (of the above two) approach produce the best result. Keep a note here, that the window size is (most probably) gonna be smaller and should be set to be equal to the largest prominent lag in PACF plot (after removing seasonality and trend). Also, special point of consideration here - I will have to inverse the results produced by adding the lag values that I have removed to get the plot of results in the desired format.\n___\n4. Create a dataset for all the stores and items. It will: (add the store and item values as features). Make the predictions using whichever approach (of the above three) produced the best results. \n    * Add the store and item values as features\n    * Employ whichever combination of the above three produces the best results.\n    * Produce the combination of test and train splits on the basis of whichever store_id and item_id is passed into the test_train_split function.\n___     \n5. (Optional) If I end up trying the clustered LSTM (for all the stores and items), I will have to create data from each store and item and structure it according to the model requirements\n","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"##### 1. The simple time series of sales of a single store","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"store_item_sale_TS_1_1 = store_item_sale_TS.copy()\ndisplay(store_item_sale_TS_1_1.head())\ndisplay(store_item_sale_TS_1_1.tail())\n\n\n# The second part of this approach to dataset will be implemented in the test train split function","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### 2. A simple 50 lag TS data (added as features/columns), to be used with windows of size 1","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Data belongs to 50 immediate lags.\nstore_item_sale_TS_2_1 = store_item_sale_TS.copy()\nfor i in range(1,50):\n    store_item_sale_TS_2_1[\"lag_{}\".format(i)] = store_item_sale_TS_2_1.sales.shift(i)\ndisplay(store_item_sale_TS_2_1.head())\ndisplay(store_item_sale_TS_2_1.tail())\n\n# Data belongs to 30 immediate lags and 30 lags from the year prior\nstore_item_sale_TS_2_2 = store_item_sale_TS.copy()\nfor i in range(1, 30):\n    store_item_sale_TS_2_2[\"lag_{}\".format(i)] = store_item_sale_TS_2_2.sales.shift(i)\nfor i in range(365, 365+30):\n    store_item_sale_TS_2_2[\"lag_{}\".format(i)] = store_item_sale_TS_2_2.sales.shift(i)\ndisplay(store_item_sale_TS_2_2.head())\ndisplay(store_item_sale_TS_2_2.tail())\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### 3. Time series with seasonality and trend components removed (using PACF plots)","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Here we will try to remove non stationarity by removing seasonality. We can see the season is an year -> 365 days.\nstore_item_sale_TS_3_1 = store_item_sale_TS.copy()\nstore_item_sale_TS_3_1 = store_item_sale_TS_3_1 - store_item_sale_TS_3_1.shift(365)\n#test_stationarity(store_item_sale_TS_3_1.iloc[365:], lags = 60)\n\nstore_item_sale_TS_3_1 = store_item_sale_TS_3_1 - store_item_sale_TS_3_1.shift(7)\n#test_stationarity(store_item_sale_TS_3_1.iloc[372:], lags = 60)\ndisplay(store_item_sale_TS_3_1.head())\ndisplay(store_item_sale_TS_3_1.tail())\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### 4. Dataset for all the stores and items","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_df = pd.read_csv('/kaggle/input/demand-forecasting-kernels-only/train.csv')\nsales_df_4_1 = sales_df.set_index(['date'])\nsales_df_4_1.index = pd.to_datetime(sales_df_4_1.index)\n# Further steps will create the dataframe in accordance to the best approached inferred from results of earlier approaches\n\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 2. Creating Test/Train Splits\nHere we are gonna break our data into test train splits and store it in the format that can be fed into the LSTM model.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def standard_scaler_LSTM(X_train, X_test):\n    X_train_scaled = []\n    X_test_scaled = []\n    scaler = StandardScaler()\n    print(\"Shape of X_train is\", X_train.shape)\n    print(\"Shape of X_test is\", X_test.shape)\n    for i in range(X_train.shape[0]):\n        X_train_scaled.append(scaler.fit_transform(X_train[i]).tolist())\n    for j in range(X_test.shape[0]):\n        X_test_scaled.append(scaler.transform(X_test[j]).tolist())\n    #display(X_train_scaled)\n    #display(X_test_scaled.shape)\n    return np.array(X_train_scaled), np.array(X_test_scaled)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def multivariate_data(dataset, target, end_index, start_index = 0, history_size = 50, target_size = 180, step = 1, single_step=False, seasonal_lag = False, slen = 365, seasonal_history_size = 30):\n    '''\n    This function converts the given data into batches that can be fed into the LSTM model\n    \n    \n    '''\n    data = []\n    data1 = []\n    label_values = []\n    if seasonal_lag:\n        start_index = start_index + slen + seasonal_history_size\n    else:\n        start_index = start_index + history_size\n        \n    print(\"Start Index is \", start_index)\n    \n    if end_index is None:\n        end_index = dataset.shape[0] - target_size\n    \n    print(\"End index is \", end_index)\n\n    for i in range(start_index, end_index):\n        \n        if seasonal_lag:\n            indices = range(i-slen-seasonal_history_size, i-slen, step)\n            data.append(dataset[indices])\n        else:\n            indices = range(i-history_size, i, step)\n            data.append(dataset[indices])\n            \n        if single_step:\n            label_values.append(target[i+target_size])\n        else:\n            label_values.append(target[i:i+target_size])\n    #print(\"Shape of label values is \", label_values.shape)\n    return np.array(data), np.array(label_values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef create_data_for_LSTM(dataset, history_size, test_size = 180, seasonal_lag = False, seasonal_history_size = 30, slen = 365):\n    '''\n    This function divides the data into test and train parts by calling the multvariate_data function. It then forms\n    \n    \n    \n    '''\n    STEP = 1\n    BATCH_SIZE = 50\n    if seasonal_lag:\n        test_start_point = dataset.shape[0] - 2*test_size - slen - seasonal_history_size\n    else:\n        test_start_point = dataset.shape[0] - 2*test_size - history_size\n    TRAIN_SPLIT = dataset.shape[0] - test_size \n    #display(TRAIN_SPLIT)\n    #display(test_start_point)\n    #display(dataset[:,0].shape)\n    print(\"Following part happens for train data generation\")\n    x_train_multi, y_train_multi = multivariate_data(dataset, dataset[:, 0], start_index = 0, end_index = TRAIN_SPLIT, history_size = history_size, seasonal_lag = seasonal_lag,seasonal_history_size = seasonal_history_size, target_size = test_size, step = STEP)\n    print(\"Following part happens for test data generation\")\n    x_test_multi, y_test_multi = multivariate_data(dataset, dataset[:, 0], start_index = test_start_point, end_index = None, history_size = history_size, seasonal_lag = seasonal_lag, seasonal_history_size = seasonal_history_size, target_size = test_size, step = STEP)\n    print ('Single window of past history : {}'.format(x_train_multi[0].shape))\n    print('Shape of x_train_multi is ', x_train_multi.shape)\n    print('Shape of x_test_multi is ', x_test_multi.shape)\n    print('Shape of y_train_multi is ', y_train_multi.shape)\n    print('Shape of y_test_multi is ', y_test_multi.shape)\n    \n    x_train_multi_scaled, x_test_multi_scaled = standard_scaler_LSTM(x_train_multi, x_test_multi)\n    \n    train_data_multi = tf.data.Dataset.from_tensor_slices((x_train_multi_scaled, y_train_multi))\n    #train_data_multi = train_data_multi.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n    train_data_multi = train_data_multi.cache().batch(BATCH_SIZE).repeat()\n\n    test_data_multi = tf.data.Dataset.from_tensor_slices((x_test_multi_scaled, y_test_multi))\n    #test_data_multi = test_data_multi.batch(BATCH_SIZE).repeat()\n    test_data_multi = test_data_multi.batch(BATCH_SIZE).repeat()\n    \n    return x_train_multi_scaled, train_data_multi, test_data_multi\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### 3. Modelling Approaches\nCreate the following iterations of models (over the vanilla LSTM) and compare the results:\n1. Add extra units.\n2. Add extra LSTM layers (remember to make sure they are spitting out sequences that can be fed into the next LSTM layer).\n3. Adjust the dropout rate, notice how it effects the output\n4. Enable Bi-Directional LSTM.\n\nIn this function I will first have to decide on the data to be used, then scale it, then convert it into numpy array, then pass it on to \"test_train_split\" part to produce the data in form of batches/tensors that can be fed into the LSTM model. Then we instantiate the model itself and train upon it.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"##### Model Implementation","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def LSTM_model_1(dataset_type = 1, test_size = 180, plot_intervals = True, plot_anomalies = False, scale = 1.25):\n    '''\n    This function defines the sequential LSTM model and implements it on the data defined by the \"dataset_type\" passed to this function\n    \n    \n    '''\n    \n    if dataset_type == 1:\n        data_df1 = store_item_sale_TS_1_1.copy()\n        data_df = store_item_sale_TS_1_1.copy()\n        data_df1 = data_df1.dropna()\n        dataset = data_df1.values\n        x_train_multi_scaled, train_data_multi, test_data_multi = create_data_for_LSTM(dataset, history_size = 20, test_size = 180, seasonal_lag = False, seasonal_history_size = 30)\n        \n    if dataset_type == 2:\n        data_df1 = store_item_sale_TS_1_1.copy()\n        data_df = store_item_sale_TS_1_1.copy()\n        data_df1 = data_df1.dropna()\n        dataset = data_df1.values\n        x_train_multi_scaled, train_data_multi, test_data_multi = create_data_for_LSTM(dataset, history_size = 30, test_size = 180, seasonal_lag = True, seasonal_history_size = 50)\n        \n    if dataset_type == 3:\n        data_df1 = store_item_sale_TS_2_1.copy()\n        data_df = store_item_sale_TS_2_1.copy()\n        data_df1 = data_df1.dropna()\n        dataset = data_df1.values\n        x_train_multi_scaled, train_data_multi, test_data_multi = create_data_for_LSTM(dataset, history_size = 2, test_size = 180, seasonal_lag = False, seasonal_history_size = 30)\n        \n    if dataset_type == 4:\n        data_df1 = store_item_sale_TS_2_2.copy()\n        data_df = store_item_sale_TS_2_2.copy()\n        data_df1 = data_df1.dropna()\n        dataset = data_df1.values\n        x_train_multi_scaled, train_data_multi, test_data_multi = create_data_for_LSTM(dataset, history_size = 5, test_size = 180, seasonal_lag = False, seasonal_history_size = 30)\n        \n    if dataset_type == 5:\n        data_df1 = store_item_sale_TS_3_1.copy()\n        data_df = store_item_sale_TS_3_1.copy()\n        data_df1 = data_df1.dropna()\n        dataset = data_df1.values\n        x_train_multi_scaled, train_data_multi, test_data_multi = create_data_for_LSTM(dataset, history_size = 30, test_size = 180, seasonal_lag = True, seasonal_history_size = 30) # This part is to be decided after evaluating the results derived from above iterations\n        \n    EPOCHS = 20\n    model_1 = tf.keras.models.Sequential()\n    model_1.add(tf.keras.layers.LSTM(64, return_sequences=True, input_shape=x_train_multi_scaled.shape[-2:]))\n    model_1.add(tf.keras.layers.LSTM(32, return_sequences=True, activation='relu'))\n    model_1.add(tf.keras.layers.Dropout(0.3))\n    model_1.add(tf.keras.layers.LSTM(16, activation='relu'))\n    #model_1.add(tf.keras.layers.Dropout(0.3))\n    model_1.add(tf.keras.layers.Dense(180))\n\n    model_1.compile(optimizer=tf.keras.optimizers.RMSprop(clipvalue=1.0), loss='mse')\n    \n    for x, y in test_data_multi.take(1):\n        print (model_1.predict(x).shape)\n        \n    model_1_history = model_1.fit(train_data_multi, epochs=EPOCHS, validation_data=test_data_multi, validation_steps=50, steps_per_epoch = 100)\n    \n    \n    \n    for x, y in test_data_multi.take(1):\n        prediction = model_1.predict(x)[0]\n    \n    \n    \n    # Now we move to the plotting part\n    \n    predicted_df = pd.DataFrame(prediction, columns = ['sales'])\n    predicted_df.index = store_item_sale_TS_test.index \n    # Total error\n    error_pred = float(mean_squared_error(data_df['sales'][0-test_size:],predicted_df['sales']))\n    \n    plt.figure(figsize = (15,7))\n    plt.title(\"Mean Squared Error {0:.2f}\".format(error_pred))\n    agg_predicted = predicted_df.resample('W', label = 'left').sum()\n    agg_actual = data_df.resample('W', label = 'left').sum()\n    plt.plot(agg_predicted, color = \"g\", label = \"Predicted Values\")\n    plt.plot(agg_actual['sales'], color = \"b\", label = \"Actual Values\")\n    \n    if plot_intervals:\n        deviation = []\n        lower_bound = []\n        upper_bound = []\n        agg_actual_reindexed = agg_actual.set_index(pd.Index(range(0,len(agg_actual))))\n        agg_predicted_reindexed = agg_predicted.set_index(pd.Index(range(0,len(agg_predicted))))\n        deviation = np.std((agg_actual_reindexed.iloc[(0-len(agg_predicted)):, 0].values, agg_predicted_reindexed['sales'].values)) \n        lower_bound = np.array(agg_predicted['sales'].values) - ((error_pred/10) + scale*deviation)\n        upper_bound = np.array(agg_predicted['sales'].values) + ((error_pred/10) + scale*deviation)\n        LowerBound_aggregated = pd.DataFrame(lower_bound, columns = ['sales'])\n        LowerBound_aggregated.index = agg_predicted.index\n        UpperBound_aggregated = pd.DataFrame(upper_bound, columns = ['sales'])\n        UpperBound_aggregated.index = agg_predicted.index\n        l1 = len(agg_predicted)\n        \n        anomalies_aggregated = np.array([np.NaN]*l1)\n        anomalies_aggregated = pd.DataFrame(anomalies_aggregated)\n        anomalies_aggregated.index = agg_predicted.index\n        drop_list1 = []\n        for j in range(len(anomalies_aggregated)):\n            if agg_actual.values[j][0] < LowerBound_aggregated.values[j][0]:\n                anomalies_aggregated.iloc[j] = agg_actual.values[j][0]\n            if agg_actual.values[j][0] > UpperBound_aggregated.values[j][0]:\n                anomalies_aggregated.iloc[j] = agg_actual.values[j][0]\n            else:\n                drop_list1.append(j)\n        anomalies_aggregated.drop(anomalies_aggregated.index[drop_list1], axis = 0, inplace = True)\n        \n        \n        plt.plot(UpperBound_aggregated['sales'], \"r--\", alpha = 0.7, label = \"Upper Bound/ Lower Bound\")\n        plt.plot(LowerBound_aggregated['sales'], \"r--\", alpha = 0.7)\n        plt.plot(anomalies_aggregated, \"o\", color = \"r\", markersize = 5, label = \"Anomalies\")\n        plt.fill_between(x = agg_predicted.index, y1 = UpperBound_aggregated.values.ravel(), y2 = LowerBound_aggregated.values.ravel(), alpha = 0.2, color = \"grey\")\n        plt.axvspan(predicted_df.index[0], predicted_df.index[-1], alpha = 0.4, color = \"grey\")\n    plt.legend(loc = 'upper left')\n    plt.grid(True)\n    plt.show()\n    \n    return model_1\n    \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model1 = LSTM_model_1()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Adding extra units and removing one of the LSTM layers","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"def LSTM_model_1(dataset_type = 1, test_size = 180, plot_intervals = True, plot_anomalies = False, scale = 1.25):\n    '''\n    This function defines the sequential LSTM model and implements it on the data defined by the \"dataset_type\" passed to this function\n    \n    \n    '''\n    \n    if dataset_type == 1:\n        data_df1 = store_item_sale_TS_1_1.copy()\n        data_df = store_item_sale_TS_1_1.copy()\n        data_df1 = data_df1.dropna()\n        dataset = data_df1.values\n        x_train_multi_scaled, train_data_multi, test_data_multi = create_data_for_LSTM(dataset, history_size = 20, test_size = 180, seasonal_lag = False, seasonal_history_size = 30)\n        \n    if dataset_type == 2:\n        data_df1 = store_item_sale_TS_1_1.copy()\n        data_df = store_item_sale_TS_1_1.copy()\n        data_df1 = data_df1.dropna()\n        dataset = data_df1.values\n        x_train_multi_scaled, train_data_multi, test_data_multi = create_data_for_LSTM(dataset, history_size = 30, test_size = 180, seasonal_lag = True, seasonal_history_size = 50)\n        \n    if dataset_type == 3:\n        data_df1 = store_item_sale_TS_2_1.copy()\n        data_df = store_item_sale_TS_2_1.copy()\n        data_df1 = data_df1.dropna()\n        dataset = data_df1.values\n        x_train_multi_scaled, train_data_multi, test_data_multi = create_data_for_LSTM(dataset, history_size = 1, test_size = 180, seasonal_lag = False, seasonal_history_size = 30)\n        \n    if dataset_type == 4:\n        data_df1 = store_item_sale_TS_2_2.copy()\n        data_df = store_item_sale_TS_2_2.copy()\n        data_df1 = data_df1.dropna()\n        dataset = data_df1.values\n        x_train_multi_scaled, train_data_multi, test_data_multi = create_data_for_LSTM(dataset, history_size = 1, test_size = 180, seasonal_lag = False, seasonal_history_size = 30)\n        \n    if dataset_type == 5:\n        data_df1 = store_item_sale_TS_3_1.copy()\n        data_df = store_item_sale_TS_3_1.copy()\n        data_df1 = data_df1.dropna()\n        dataset = data_df1.values\n        x_train_multi_scaled, train_data_multi, test_data_multi = create_data_for_LSTM(dataset, history_size = 30, test_size = 180, seasonal_lag = True, seasonal_history_size = 30) # This part is to be decided after evaluating the results derived from above iterations\n        \n    EPOCHS = 25\n    model_1 = tf.keras.models.Sequential()\n    model_1.add(tf.keras.layers.LSTM(120, return_sequences=True, input_shape=x_train_multi_scaled.shape[-2:]))\n    model_1.add(tf.keras.layers.LSTM(64, return_sequences=True, activation='relu'))\n    model_1.add(tf.keras.layers.Dropout(0.2))\n    model_1.add(tf.keras.layers.LSTM(16, activation='relu'))\n    #model_1.add(tf.keras.layers.Dropout(0.3))\n    model_1.add(tf.keras.layers.Dense(180))\n\n    model_1.compile(optimizer=tf.keras.optimizers.RMSprop(clipvalue=1.0), loss='mse')\n    \n    for x, y in test_data_multi.take(1):\n        print (model_1.predict(x).shape)\n        \n    model_1_history = model_1.fit(train_data_multi, epochs=EPOCHS, validation_data=test_data_multi, validation_steps=50, steps_per_epoch = 100)\n    \n    \n    \n    for x, y in test_data_multi.take(1):\n        prediction = model_1.predict(x)[0]\n    \n    \n    \n    # Now we move to the plotting part\n    \n    predicted_df = pd.DataFrame(prediction, columns = ['sales'])\n    predicted_df.index = store_item_sale_TS_test.index \n    # Total error\n    error_pred = float(mean_squared_error(data_df['sales'][0-test_size:],predicted_df['sales']))\n    \n    plt.figure(figsize = (15,7))\n    plt.title(\"Mean Squared Error {0:.2f}\".format(error_pred))\n    agg_predicted = predicted_df.resample('W', label = 'left').sum()\n    agg_actual = data_df.resample('W', label = 'left').sum()\n    plt.plot(agg_predicted, color = \"g\", label = \"Predicted Values\")\n    plt.plot(agg_actual['sales'], color = \"b\", label = \"Actual Values\")\n    \n    if plot_intervals:\n        deviation = []\n        lower_bound = []\n        upper_bound = []\n        agg_actual_reindexed = agg_actual.set_index(pd.Index(range(0,len(agg_actual))))\n        agg_predicted_reindexed = agg_predicted.set_index(pd.Index(range(0,len(agg_predicted))))\n        deviation = np.std((agg_actual_reindexed.iloc[(0-len(agg_predicted)):, 0].values, agg_predicted_reindexed['sales'].values)) \n        lower_bound = np.array(agg_predicted['sales'].values) - ((error_pred/10) + scale*deviation)\n        upper_bound = np.array(agg_predicted['sales'].values) + ((error_pred/10) + scale*deviation)\n        LowerBound_aggregated = pd.DataFrame(lower_bound, columns = ['sales'])\n        LowerBound_aggregated.index = agg_predicted.index\n        UpperBound_aggregated = pd.DataFrame(upper_bound, columns = ['sales'])\n        UpperBound_aggregated.index = agg_predicted.index\n        l1 = len(agg_predicted)\n        \n        anomalies_aggregated = np.array([np.NaN]*l1)\n        anomalies_aggregated = pd.DataFrame(anomalies_aggregated)\n        anomalies_aggregated.index = agg_predicted.index\n        drop_list1 = []\n        for j in range(len(anomalies_aggregated)):\n            if agg_actual.values[j][0] < LowerBound_aggregated.values[j][0]:\n                anomalies_aggregated.iloc[j] = agg_actual.values[j][0]\n            if agg_actual.values[j][0] > UpperBound_aggregated.values[j][0]:\n                anomalies_aggregated.iloc[j] = agg_actual.values[j][0]\n            else:\n                drop_list1.append(j)\n        anomalies_aggregated.drop(anomalies_aggregated.index[drop_list1], axis = 0, inplace = True)\n        \n        \n        plt.plot(UpperBound_aggregated['sales'], \"r--\", alpha = 0.7, label = \"Upper Bound/ Lower Bound\")\n        plt.plot(LowerBound_aggregated['sales'], \"r--\", alpha = 0.7)\n        plt.plot(anomalies_aggregated, \"o\", color = \"r\", markersize = 5, label = \"Anomalies\")\n        plt.fill_between(x = agg_predicted.index, y1 = UpperBound_aggregated.values.ravel(), y2 = LowerBound_aggregated.values.ravel(), alpha = 0.2, color = \"grey\")\n        plt.axvspan(predicted_df.index[0], predicted_df.index[-1], alpha = 0.4, color = \"grey\")\n    plt.legend(loc = 'upper left')\n    plt.grid(True)\n    plt.show()\n    \n    return model_1\n    \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}